{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\" # put your full name here\n",
    "COLLABORATORS = [] # list anyone you collaborated with on this workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 8: Model selection and regularization\n",
    "\n",
    "**This lab was distributed Monday 10/21/2019 and should be completed by Monday 10/28/2019 at 11:59PM.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the eighth lab of the semester!\n",
    "\n",
    "In this lab, we'll cover model selection and regularization (ISLR 6.1-6.2). Before we get into that, let's review what we've done so far in terms of modeling:\n",
    "* in lab 5, we covered linear regression by using ordinary least-squares (OLS) optimization to find the intercept and slope coefficient for a a linear relationship\n",
    "* in lab 7, we covered gradient descent, which is a way to find the coefficients of a model (in our case, a linear model) by iteratively calculating the coefficients using the gradient (the partial derivative of the model loss with respect to the coefficient) until we find a set of coefficients that minimize the loss function\n",
    "* also in lab 7, we covered cross-validation, which is a way to find out how well a model performs with new data\n",
    "\n",
    "Model selection and regularization relate in some way to all of these topics. Model selection and regularization approaches give us a method to select which variables (i.e. features) to include in our model, as well as to select the value of the oefficients that should be associated with those variables. \n",
    "\n",
    "Cross-validation, which we covered most recently, also allows us to select variables by comparing the cross-validation error of different models that include different number of variables (homework 7 is a good example of this). However, while cross-validation works by running a separate cross-validation for each potential combination of variables, the model selection approaches (like subset selection) covered in ISLR 6.1-6.2 work by iteratively and strategically adding variables to the model until the \"best\" model is achieved.\n",
    "\n",
    "Model selection and regularization are also related to OLS in that they provide an alternative approach to choosing model coefficients. Rather than just minimizing the loss, techniques like ridge or lasso regression add a penalty term that penalizes the model if it's too large, or includes too many features, by pushing some of the coefficients to zero or close to zero.\n",
    "\n",
    "Regularization methods like ridge or lasso can also work *with* cross-validation - for instance, in this lab, we're going to see what happens when we change the tunable parameter $\\lambda$ in ridge and lasso regression. We're going to see how different values of $\\lambda$ perform on a random train-test split of the data. Although we don't do it in this lab, a more systematic and rigorous way of evaluating different values of $\\lambda$ would be to see how different values of $\\lambda$ perform using leave one out or k-fold cross validation.\n",
    "\n",
    "In this lab, we'll focus on ridge and lasso regression, focusing on how to implement them and examining their prediction error and the coefficients that result from using these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: EDA and data filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be working with the Novotny et al. land-use regression dataset used in HW5.  Here's a refresher about the data:\n",
    "\n",
    "* The data is an accumulation of GIS land-use characteristics from EPA land-monitoring, and in situ NO2 measurements from satellite sensors.\n",
    "* The goal of land-use regression (LUR) is to estimate outdoor air pollution geospatially across the contiguous United States.\n",
    "* The reason for the high number of data points is that the data keeps track of readings from monitors at a high resolution, up to ~30 meters.\n",
    "\n",
    "We will only be working with a small subset of this data in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run to load the dataset we'll be working with\n",
    "df = pd.read_csv('data/BechleLUR_2006_allmodelbuildingdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1** What is column name of the target (response) variable for our model? Display the Pandas series containing the response variable.\n",
    "\n",
    "Reminder: the target variable will allow us to estimate atmospheric $NO_2$ levels at different points in space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2** Create a dataframe `df_model` that contains only the response and predictor variables (i.e. you should drop Monitor_ID, State, Latitude, Longitude, and Predicted_NO2_ppb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3** We have a lot of potential features in our dataset, and it's hard to visualize all of them in relation to our response variable. To gain some familiarity with the data, however, create a plot with 4 subplots below, and generate 4 scatterplots, each showing a different potential feature on the x-axis and the response variable on the y-axis. Do you observe any trends or relationships? Visually, would you expect a model selection algorithm to prioritize or minimize any of these features? Why?\n",
    "\n",
    "The code below is mostly written for you - you just need to choose four features to plot. You're welcome to change the formatting if you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Observed_NO2_ppb'] # y axis variable\n",
    "ylab = \"Observed atmospheric $NO_2$ (ppb)\" # y axis label\n",
    "\n",
    "msize = 70 # marker size\n",
    "afsize = 15 # axis font size\n",
    "tfsize = 20 # title font size\n",
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.scatter(..., y, s = msize)\n",
    "plt.xlabel(..., fontsize = afsize)\n",
    "plt.ylabel(ylab, fontsize = afsize)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.scatter(..., y, s = msize)\n",
    "plt.xlabel(..., fontsize = afsize)\n",
    "plt.ylabel(ylab, fontsize = afsize)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.scatter(..., y, s = msize)\n",
    "plt.xlabel(..., fontsize = afsize)\n",
    "plt.ylabel(ylab, fontsize = afsize)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.scatter(..., y, s = msize)\n",
    "plt.xlabel(..., fontsize = afsize)\n",
    "plt.ylabel(ylab, fontsize = afsize)\n",
    "\n",
    "plt.suptitle(..., fontsize = tfsize)\n",
    "\n",
    "plt.subplots_adjust(top=0.5) # avoid overlapping title and plots\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that we've loaded and done some basic exploration of the data, we can think about how to choose which features to include in the model. Features can provide important information and predictive power. However, adding features to the data means we risk increasing the variance of our model (meaning our model performs poorly with test data relative to training data) and also reduces the interpretability of a model (it's harder to make sense of a model with lots of features). Rather than throwing out features entirely, we can turn to a technique called regularization to reduce the variance of our model while still incorporating as much information about the data as possible.\n",
    "\n",
    "More generally, we can adopt the framework of regularized loss minimization.\n",
    "\n",
    "$$ \\large \\hat{\\theta} = \\arg \\min_\\theta \\frac{1}{n} \\sum_{i=1}^n \\textbf{Loss}\\left(y_i, \\hat{y_i}\\right) + \\lambda \\textbf{R}(\\theta) $$\n",
    "\n",
    "The regularization term $\\textbf{R}(\\theta)$ penalizes for $\\theta$ values that result in more complex and therefore higher variance models. The regularization parameter $\\lambda$ determines the degree of regularization to apply and is typically determined through cross validation.\n",
    "\n",
    "The two regularlization methods that we're exploring in this lab (ridge regression and lasso regression) use different functions for the loss function and for the regularization term $\\textbf{R}(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: L2 Regularization with Ridge Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_2$ regularization is a method of penalizing large weights in a cost function in order to lower model variance.\n",
    "\n",
    "Ridge regression (L2 regularization) uses the *penalty* term $\\large R_{L^2}(\\theta) = \\sum_{k=1}^p (\\theta_k)^2$, where $p$ is the number of model features.\n",
    "\n",
    "Note that $\\lambda$ is a tunable parameter - as the person creating the model, you can choose to increase or decrease $\\lambda$ based on how much you want to penalize the addition of model features. The higher the value of $\\lambda$, the more a model is penalized on its higher order terms. This penalization decreases the model's variance at the cost of increasing its bias.\n",
    "\n",
    "In scikit-learn, the value of $\\lambda$ is passed in through the argument `alpha` as follows:\n",
    "$$\\alpha = \\frac{1}{\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Question 2.1** Separate the `df_model` dataframe into train and test sets, with 20% of the data in the test set. Begin by setting `X` to the matrix of predictor variables (all quantitative columns in the dataframe except the response variable) and set `y` equal to the response variable `Observed_NO2_ppb`.Then apply `train_test_split` to `X` and `y` to split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = ...\n",
    "y = ...\n",
    "\n",
    "X_train, X_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this to make sure you split the data correctly\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2** Import and create a Ridge regression model with `alpha` value set to 1. Fit the training data into the model, then return a list of the coefficients that the model predicts for each feature in the training data. The [scikit-learn documentation for Ridge()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) is helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = ...\n",
    "ridge.fit(...)\n",
    "ridge_coefficients = ...\n",
    "\n",
    "print(ridge_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3** Now fit a `LinearRegression` model without regularization and print the resulting list of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm = ...\n",
    "lm.fit(...)\n",
    "lm_coefficients = ...\n",
    "\n",
    "print(lm_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4** Run the code below to generate a bar chart that shows the coefficient values from simple linear regression in blue, and from ridge regression in red. Then, in the markdown cell below, comment on the results. Can you explain your observations based on your understanding of L2 ridge regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "ind = np.arange(len(lm_coefficients))\n",
    "width = 0.5\n",
    "\n",
    "plt.figure(figsize = (15,7))\n",
    "\n",
    "plt.bar(ind-(width/2), width = width, height = lm_coefficients, label = \"simple linear regression\")\n",
    "plt.bar(ind+(width/2), width = width, height = ridge_coefficients, label = r\"ridge regression, $\\alpha$ = 1\")\n",
    "plt.xlabel(\"feature number\")\n",
    "plt.ylabel(\"coefficient\")\n",
    "plt.title(\"Coefficient values with simple linear regression and ridge regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5**: We just observed how the Ridge Regression model generates coefficients when `alpha` is set to one. Complete the following code which generalizes the fitting and predicting process we just did in Question 2.3 for various values of `alpha`.\n",
    "\n",
    "Then calculates the mean squared error (MSE) between our predictions and the test dataset. The MSE in this case is a measure of the accuracy of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "alphas = [1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9]\n",
    "mses = []\n",
    "\n",
    "for a in alphas:\n",
    "    model = ...\n",
    "    model.fit(...)\n",
    "    y_pred = model.predict(...)\n",
    "    mses.append(mean_squared_error(y_pred, y_test))\n",
    "\n",
    "print(mses)\n",
    "\n",
    "a_log = np.log10(alphas)\n",
    "\n",
    "plt.figure(figsize = (15,5))\n",
    "sns.barplot(a_log, mses, color = 'cadetblue')\n",
    "plt.xlabel(r'$log_{10}(\\alpha)$')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Ridge regression MSE for each value of alpha');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.6** What is the lowest MSE observed and which value of alpha did it come from? What value of $\\lambda$ does that correspond to? Does the value of $\\lambda$ that minimizes MSE more heavily or less heavily penalize additional coefficients than our initial value of $\\lambda$ that we used to produce the plot in question 2.4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.7** How does ridge regression using the value of `alpha` identified in question 2.6 perform relative to simple linear regression with respect to the mean squared error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: L1 Regularization with Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While ridge regression minimizes coefficients, it incorporates *all* the features into your model. It won't actually drive any coefficients to 0 (unless $\\lambda$ = $\\infty$!). This can make your model less *interpretable* - for instance, in the case of the model we created in Section 2, we have over 130 non-zero coefficients and thus over 130 features.\n",
    "\n",
    "Lasso regression (also called L1 Regularization) avoids the issue of including too many unimportant variables by using a model formulation that can drive some coefficients to 0.\n",
    "\n",
    "Lasso regression uses the *penalty* term $\\large R_{L^1}(\\theta) = \\sum_{k=1}^p \\Big|\\theta_k\\Big|$, where $p$ is the number of model features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1** Let's repeat the steps we did above for Ridge Regression, this time for Lasso Regression. Create a Lasso model with an `alpha` of 1 and fit on the X_train and y_train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = ...\n",
    "lasso.fit(...)\n",
    "lasso_coefficients = ...\n",
    "print(lasso_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2** Output a plot that shows the coefficients from the simple linear regression in part 2, the ridge regression in part 2, and the lasso regression above side-by-side. You can adapt the code from question 2.4 or write your own, and can choose whatever plot format makes the most sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3:** Comment on the results in question 3.2. Can you explain your observations based on your understanding of L1 lasso regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4** What proportion of the datasets features are \"ignored\" by this lasso model? What are the column names of the features that are **not** ignored by this lasso model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.5**: Look back to the features you plotted in question 1.3. Were any of those features ignored or included by the lasso model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.6**: Remember how we calculated the test MSE for different values of $\\alpha$ in question 2.5? Now, we're going to write a function that automates that process, taking as input a list of alphas `alphas` and a model (`Ridge` or `Lasso`). Complete the function below, and then define a list of alphas and call the function using the `Lasso` model to return a list of MSEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mses(alphas, Model):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        alphas (array): contains floats of various alpha values\n",
    "        Model (sklearn model): the type of sklearn model on which to fit the data\n",
    "    Output:\n",
    "        an array of floats containing the mean-squared-errors from the predictions\n",
    "    \"\"\"\n",
    "    mses = []\n",
    "\n",
    "    for a in alphas:\n",
    "        # Your code here\n",
    "        \n",
    "    return mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output lasso MSEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.7**: How well does the Lasso Regression model perform against the Ridge Regression model from before? Calculate the ridge MSEs and the lasso MSEs using the same set of `alphas` then plot the two series against each other using whatever type of plot makes the most sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = ...\n",
    "lasso_mses = ...\n",
    "ridge_mses = ...\n",
    "\n",
    "# plot lasso vs ridge MSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.8** Explain the plot we generated above. Which model performs more consistently on the test data across various values of alpha? Why might this be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hooray, you're done! \n",
    "\n",
    "Please remember to submit your lab work, after clicking Kernel -> Restart & Run All, in .html and .ipynb format on bCourses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Reading:\n",
    "\n",
    "Regularization - https://www.textbook.ds100.org/ch/16/reg_intro.html\n",
    "    \n",
    "Notebook developed by Alex McMurry, Kevin Marroquin, and Melissa Ly\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
