{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\" # put your full name here\n",
    "COLLABORATORS = [] # list names of anyone you worked with on this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ERG 131] Homework 8: Model Selection and Regularization\n",
    "\n",
    "This assignment explores methods that answer the question of how to choose which features to include in a model. In HW7 (resampling), we used cross-validation to evaluate how well different models generalize to new data, and one of the big takeaways was that using too many features can cause us to over-fit our model on the training data, and lead to a model that performs poorly on test data. In this HW, we'll look at two methods that attempt to reduce model variance by prioritizing certain model features: ridge and lasso regression.\n",
    "\n",
    "A good reference for this HW is ISLR Ch. 6.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [The data](#data)<br>\n",
    "1. [Comparing linear, ridge, and lasso regression](#models)<br>\n",
    "1. [Choosing lambda](#lambda)<br>\n",
    "1. [Comparing optimal models](#compare)<br>\n",
    "1. [Project](#proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T18:50:14.728090Z",
     "start_time": "2018-08-13T18:50:08.632835Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run this block\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: The Data<a name='data'></a>\n",
    "\n",
    "We're going to be working (for the third and final time!) with the Novotny et al. LUR data. You will recognize it from HW5 and lab 8. As a refresher:\n",
    "\n",
    "* The data is an accumulation of GIS land-use characteristics from land-monitoring from the EPA, and in situ NO2 measurements from satellite sensors.\n",
    "* The goal of land-use regression (LUR) is to estimate outdoor air pollution geospatially across the contiguous United States.\n",
    "* The reason for the high number of data points is that the data keeps track of readings from monitors at a high resolution, up to ~30 meters.\n",
    "\n",
    "First, let's upload the data to dataframe `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "df = pd.read_csv('data/BechleLUR_2006_allmodelbuildingdata.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we're going to be creating models for two sets of data: one for the full dataset, and one for just California data. We'll be looking at how different approaches (subset selection, lasso, and ridge) perform with datasets that have more or less observations.\n",
    "\n",
    "**Question 1.1** To get started, create a dataframe `df_ca` that contains just California observations from dataframe `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_ca.shape == (94,135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2** As with the lab, we want to predict Observed_NO2_ppb and we want to use all the columns except for Monitor_ID, State, Latitude, Longitude, Observed_NO2_ppb, and Predicted_NO2_ppb as features.\n",
    "\n",
    "Write a function `get_X_y()` that takes as input a dataframe, and returns four dataframes: X_train, X_test, y_train, and y_test. Remember to start by selecting only the X values (all the columns except for the ones listed above) and the one y value (observed NO2). Then, call that function on `df` and `df_ca`, and save the resulting dataframes to `X_all_train, X_all_test, y_all_train, y_all_test` and `X_ca_train, X_ca_test, y_ca_train, y_ca_test` respectively.\n",
    "\n",
    "Write your function to hold 20% of the data as test data, with a `random_state` of 123456 as input to `train_test_split()`.  This is important!  The assert functions won't work if you don't use that random state.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_X_y(df):\n",
    "    \"\"\"\n",
    "    This function returns two dataframes containing the X and y values used in land-use regression.\n",
    "    Input: df, a Pandas dataframe with all of the fields in the land-use regression dataset\n",
    "    Returns: X_train, X_test, y_train, y_test, four dataframes containing the training and testing subsets of the \n",
    "    feature matrix X and response matrix y\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_all_train, X_all_test, y_all_train, y_all_test = ... # get X and y train and test dataframes for full dataset\n",
    "X_ca_train, X_ca_test, y_ca_train, y_ca_test = ... # get X and y train and test dataframes for CA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dimensions of output dataframes\n",
    "assert X_ca_train.shape[0] == y_ca_train.shape[0]\n",
    "assert X_all_test.shape[0] == y_all_test.shape[0] == round(X_all_train.shape[0]/4)\n",
    "assert X_ca_test.shape[1] == X_all_test.shape[1] == 129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Comparing linear, ridge, and lasso regression<a name='models'></a>\n",
    "\n",
    "Our feature matrix has a very high number of features in it (129 to be exact). It would be a big hassle to try and determine the weight of importance from each variable (that would take ages!). So how should we do it?\n",
    "\n",
    "As a recap from lab 8 and from lecture, for a normal regression, a model with two features would have a model that looks like this:\n",
    "\n",
    "$$\\Large \\hat{y} = \\theta_1 x_{1i} + \\theta_2 x_{2i}$$\n",
    "\n",
    "However, there is the possibility such that one of the indicator variables does not have as much of an impact on the value of the predicted variable as originally intended when tested on unseen data. Conversely, there could also be similar variables that are not given equal weight. This would cause an overfitting problem later, meaning that the graph would have high variance. How would we fix this problem?\n",
    "\n",
    "Enter the **ridge regression ($L^2$ regularization)**. This formula takes the minimization of mean squared error that we do with linear regression formula and adds a **regularization** or **penalty** term. Visually...\n",
    "\n",
    "$$ \\large \\hat{\\theta} = \\arg \\min_\\theta \\frac{1}{n} \\sum_{i=1}^n \\textbf{Loss}\\left(y_i, \\hat{y_i}\\right) + \\lambda {R_{L^2}}(\\theta) $$\n",
    "\n",
    "where $\\large R_{L^2}(\\theta) = \\sum_{k=1}^p (\\theta_k)^2$ \n",
    "\n",
    "In the case of Lasso regression, we would uses the penalty term $\\large R_{L^1}(\\theta) = \\sum_{k=1}^p \\Big|\\theta_k\\Big|$, where $p$ is the number of model features.\n",
    "\n",
    "The higher the value of $\\large \\lambda$, the more a model is penalized for reaching outsider values. Essentially, we are decreasing variance, and increasing bias with our higher lambda value.\n",
    "\n",
    "For the purposes of `sklearn`, the lambda value will be passed in as an argument `alpha`, where $$\\alpha = \\lambda$$\n",
    "\n",
    "However, what value of $\\lambda$ would be a good sized penalty term? We need to check a few answers to find a good option for this term. What is a technique that we know that can will check for the \"best fit\" across different terms? You guessed it: cross-validation! We'll be doing cross validation in the next section. For now, let's see how our models perform on a random train-test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1** In this homework we're going to compare the results of three models: a simple linear regression, ridge regression, and lasso regression. As we know from lab 8, `scikit-learn` has a very generalizable syntax when it comes to using different models, where you instantiate a model and save that instance of the model to a variable (eg. `lm = LinearRegression()`), then use `.fit()` to fit that model to the training data and `.predict()` to output predictions for test data.\n",
    "\n",
    "In this question, we're going to complete a function `fit_model()` that automates the process of initializing a model, fitting that model, and getting predictions. The function should return two sets of values: the model coefficients, and the test mean squared error of the model.\n",
    "\n",
    "We've gotten you started with some skeleton code. It may be helpful to test out each part of the function separately, and then put it all together. It also may be helpful to look back at lab 8 to see how we initialize and fit different models.\n",
    "\n",
    "*Hint:* When getting the coefficients of a model using `.coef_`, sometimes scikit-learn returns a 1D array and sometimes it returns a nested array (i.e. with double brackets). To make sure that the dimensions of the coefficients that you output using this function are always the same, consider using the [numpy .flatten() method](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html).\n",
    "\n",
    "*Another hint:* We're fitting one of three potential models here: LinearRegression(), Ridge(), and Lasso(). While Ridge() and Lasso() take in an alpha argument, LinearRegression() does not! That means your code has to check what type of model you'll be fitting before initializing that model and passing an alpha argument to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def fit_model(Model, X_train, X_test, y_train, y_test, alpha = 1):\n",
    "    \"\"\"\n",
    "    This function fits a model of type Model to the data in the training set of X and y, and finds the MSE on the test set\n",
    "    Inputs: \n",
    "        Model (sklearn model): the type of sklearn model with which to fit the data - LinearRegression, Ridge, or Lasso\n",
    "        X_train: the set of features used to train the model\n",
    "        y_train: the set of response variable observations used to train the model\n",
    "        X_test: the set of features used to test the model\n",
    "        y_test: the set of response variable observations used to test the model\n",
    "        alpha: the penalty parameter, to be used with Ridge and Lasso models only\n",
    "    Returns:\n",
    "        mse: a scalar containing the mean squared error of the test data\n",
    "        coeff: a list of model coefficients\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize model\n",
    "        \n",
    "    # fit model\n",
    "    \n",
    "    # get mean squared error of test data\n",
    "    mse = ...\n",
    "    \n",
    "    # get coefficients of model\n",
    "    coef = ...\n",
    "    \n",
    "    return mse, coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to confirm that fit_model() works as expected\n",
    "\n",
    "# check the MSE of using Ridge regression on the full data - the value should be ~ 17 if your random_state was 123456 in get_X_y\n",
    "assert 17 < fit_model(Ridge, X_all_train, X_all_test, y_all_train, y_all_test)[0] < 18\n",
    "\n",
    "# make sure that the number of coefficients is equal to the number of features\n",
    "assert len(fit_model(LinearRegression, X_all_train, X_all_test, y_all_train, y_all_test)[1]) == 129\n",
    "\n",
    "# check that Lasso is reducing the coefficients as expected - i.e. by setting some coefficients to zero\n",
    "assert np.count_nonzero(fit_model(Lasso, X_all_train, X_all_test, y_all_train, y_all_test)[1]) < len(fit_model(Lasso, X_all_train, X_all_test, y_all_train, y_all_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2** For each set of data (the training and testing sets of `X_all, y_all` and `X_ca, y_ca` obtained in question 1.2), call `fit_model()` 3 times: once with `Model = LinearRegression`, once with `Model = Ridge` and once with `Model = Lasso`. For ridge and lasso, you can leave the `alpha` value as its default value (1). Save both the MSE and the coefficient output everytime you run the models. You'll end up with 6 MSE values and 6 sets of coefficients (3 for the full dataset and 3 for California; 1 per model type in each case). It's up to you how you store the values (you can use lists or arrays or separate variable names, as long as you're able to access the MSE values and coefficients later in the lab). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3** Create a plot with two subplots below. In the first subplot, plot the coefficients for ridge regression, lasso regression, and the linear model for the full dataset. In the second, plot the same but for the California data. You can use whatever type of plot makes the most sense. Make sure to have axis labels, titles, and legends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4** How do the coefficients from lasso, ridge, and linear regression compare for the full dataset? How about for the California data? Based on your knowledge of lasso, ridge, and linear regression, can you explain why the coefficients compare in this way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5** Compare the 3 test MSES from the full dataset and from the California dataset. How do the MSEs compare within model types (i.e. how does the MSE for linear regression fit on the California model compare with that fit on the full model)? Can you explain how the two sets of values compare, based on what you know about bias and variance? \n",
    "\n",
    "Particularly, note how the MSEs change for the California data when Lasso is used vs. for all data when Lasso is used. Which set of data exhibits a larger improvement when Lasso is used? Why do you think that is?\n",
    "\n",
    "You don't have to use a visualization for this question, although you are welcome to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell for scratch work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Choosing lambda<a name='lambda'></a>\n",
    "\n",
    "Now that you've gotten the chance to play with the \"penalty\" term of a simple regression, we have two ways in our toolbelt to combat the danger of overfitting our data: regularization and cross-validation (hw7). To further tune our model for maximum success rate, we'll combine these two approaches to find the ideal $\\lambda$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.1:** In this question, you'll complete a function `model_cv_mse()`. This function should perform k-fold cross validation on the training data, which is passed to the function through `X` and `y`, using the model (`Ridge()` or `Lasso()`) specified by the input `Model` for each value of alpha in the list `alphas`. It should then calculate the MSE for each value of alpha in `alphas`, and return a list of mean squared errors.\n",
    "\n",
    "This time, use a `random_state` of 0 for `KFold`, and set `shuffle = True`.\n",
    "\n",
    "This function will end up looking a lot like `mse_k_fold_lr()` from homework 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cv_mse(Model, X, y, alphas, k = 3):\n",
    "    \"\"\"\n",
    "    This function calculates the MSE resulting from k-fold CV using lasso regression performed on a training subset of \n",
    "    X and y for different values of alpha.\n",
    "    Inputs: \n",
    "        Model (sklearn model): the type of sklearn model with which to fit the data - Ridge or Lasso\n",
    "        X: the set of features used to fit the model\n",
    "        y: the set of response variable observations\n",
    "        alphas: a list of penalty parameters\n",
    "        k: number of folds in k-fold cross-validation\n",
    "    Returns:\n",
    "        average_mses: a list containing the mean squared cross-validation error corresponding to each value of alpha\n",
    "    \"\"\"\n",
    "    mses = ... # initialize array of MSEs to contain MSE for each fold and each value of alpha\n",
    "        \n",
    "    kf = KFold(...) # get kfold split\n",
    "    \n",
    "    fold = 0\n",
    "    for train_i, val_i in kf.split(X):\n",
    "        # get training and validation values\n",
    "        X_f_train = ...\n",
    "        X_f_val = ...\n",
    "        y_f_train = ...\n",
    "        y_f_val = ...\n",
    "        \n",
    "        for i in range(len(alphas)): # loop through alpha values\n",
    "            model = ... # initialize model\n",
    "\n",
    "            model.fit(...) # fit model\n",
    "            \n",
    "            y_pred = ... # get predictions\n",
    "            \n",
    "            # save MSE for this fold and alpha value\n",
    "        \n",
    "        fold += 1\n",
    "    \n",
    "    average_mses = ... # get average MSE for each alpha value across folds\n",
    "    \n",
    "    return average_mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "alphas_test = [0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "# check dimensions of output MSE\n",
    "assert len(model_cv_mse(Ridge, X_all_train, y_all_train, alphas_test)) == len(alphas_test)\n",
    "\n",
    "# check values of output MSE at first alpha - should be about 13.4\n",
    "assert 13 < model_cv_mse(Lasso, X_all_train, y_all_train, alphas_test)[0] < 14\n",
    "\n",
    "# check reproducibility - the function should give the same MSE for the same alphas every time it runs\n",
    "assert np.array_equal(model_cv_mse(Ridge, X_all_train, y_all_train, alphas_test),\n",
    "                       model_cv_mse(Ridge, X_all_train, y_all_train, alphas_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.2** Now that we can calculate the cross-validated MSE for differen values of alpha, we can visualize the relationship between alpha and cross-validated MSE. Below, create two subplots: one that shows alpha vs. cross-validated MSE for ridge regression for the training portion of the full dataset, and one that shows alpha vs cross-validated MSE for lasso regression for the training portion of the full dataset. Use 3-fold CV.\n",
    "\n",
    "It's up to you what range of alphas you want to show, but your plot should aim to roughly include the range around which the minimum MSE value appears. As a starting point, look at the output MSE values using the `test_alphas` values in the `assert` code block above, and try to decide which alphas to \"zoom in\" on from there - you may also decide to extend the range of alphas. It may take some trial and error.\n",
    "\n",
    "*Hint*: You will end up with two very different ranges of alpha for lasso and ridge. The lasso plot will look more like a convex function, with a clear minimum, but the ridge plot will be more \"L\" shaped, where at large enough values of alpha the MSE stays relatively constant.\n",
    "\n",
    "Make sure to add axis labels and titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.3** Comment on the plots above. How does the MSE change with different alphas for ridge vs lasso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.4** Now, repeat the plotting exercise in question 3.3 but use the California-only data. You'll create two subplots: one that shows alpha vs. cross-validated MSE for ridge regression for the training portion of the California dataset, and one that shows alpha vs cross-validated MSE for lasso regression for the training portion of the California dataset. Use 3-fold CV.\n",
    "\n",
    "As with question 3.3, you'll want to use trial and error to figure out what range of alphas to plot for each model. Similar to question 3.3, your ridge plot will be \"L\" shaped, and your lasso plot will be more convex.\n",
    "\n",
    "Make sure to add axis labels and titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.5** Comment on the plots in question 3.5. How do they differ from the plots in question 3.3? In particular, think about the following questions: \n",
    "* is there a larger relative improvement in MSE when alpha increases with models fit to the full dataset, or the California dataset? Why might that be?\n",
    "* How does the roughly optimal alpha value for Lasso regression for the model fit to the California data compare to the roughly optimal alpha value for Lasso regression for the model fit to the full dataset? Can you explain the difference between the values? (of course, different datasets will lead to different optimal alphas - but try to go one step beyond that explanation and think about the role that sample size and variance might be playing here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.6:** Now that we've seen both lasso and ridge regression in use, how do ridge regression and the lasso improve on ordinary least squares? In what cases would you expect ridge regression to perform better than lasso, and vice versa? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Comparing optimal models<a name='compare'></a>\n",
    "\n",
    "In this section, we'll compare the performance of three models: your Lasso model with optimal alpha, your Ridge model with optimal alpha, and Novotny's model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.1:** Let's start by getting the optimal alpha and corresponding coefficients of the Ridge model. In this section, we'll use a new scikit-learn function, [`RidgeCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html). `RidgeCV()` takes as an input a list of alpha values and a cross-validation splitter object (which is just the output of the function `KFold(...)`).\n",
    "\n",
    "To call `RidgeCV()`, you would want to first create a cross-validation object using `Kfold()`:\n",
    "\n",
    "`kf = KFold(n_splits = ..., shuffle = ..., random_state = ...)`\n",
    "\n",
    "You can then pass `kf` to `RidgeCV()`, along with a list of alpha values:\n",
    "\n",
    "`ridgecv = RidgeCV(cv = kf, alphas = ...)`\n",
    "\n",
    "and you can treat it essentially like any other `scikit-learn` model by calling `.fit()`:\n",
    "\n",
    "`ridgecv.fit(X_train, y_train)`\n",
    "\n",
    "Running the code above will, in one line, perform cross-validation, choose an ideal alpha from your list, and select the corresponding coefficients. You can then access the optimal alpha using:\n",
    "\n",
    "`ridgecv.alpha_`\n",
    "\n",
    "and the corresponding coefficients using:\n",
    "\n",
    "`ridgecv.coef_`\n",
    "\n",
    "You can also get predictions on your test data using `.predict()`:\n",
    "\n",
    "`ridgecv.predict(X_test)`\n",
    "\n",
    "In the cell below, we've written some skeleton code to get you started. You'll want to call `RidgeCV()` using 3-fold cross validation with a random state of 0, train the model on the training data for the **full** dataset (i.e. `X_all_train, y_all_train`), get predictions for the test data (i.e. `X_all_test`), and find the mean-squared-error on your cross-validated ridge model on the test data. Save the mean squared error to variable `ridgecv_mse`.\n",
    "\n",
    "You may also want to print the optimal alpha value to make sure it's consistent with your plot in question 3.3.\n",
    "\n",
    "You can use the same range of alphas that you did for plotting, or you can choose to use a smaller, larger, or more granular range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "kf = KFold(...) # get KFold cross-validation selector object\n",
    "ridgecv = RidgeCV(cv = ..., alphas=...) # pass CV object and list of alphas to RidgeCV()\n",
    "ridgecv.fit(...) # fit RidgeCV model on training data\n",
    "\n",
    "alpha_opt = ... # get optimal alpha value\n",
    "print(\"optimal alpha:\", alpha_opt)\n",
    "\n",
    "y_pred_ridgecv = ... # get test predictions using RidgeCV model\n",
    "\n",
    "ridgecv_mse = ... # get MSE of RidgeCV model\n",
    "print(\"Test MSE with cross-validated Ridge:\", ridgecv_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.2:** Now, let's get the optimal alpha and corresponding coefficients of the Lasso model. We'll use the function [`LassoCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html). As in question 4.1, `LassoCV()` takes as an input a list of alpha values and a cross-validation splitter object (which is just the output of the function `KFold(...)`). You would call `LassoCV()` and use it to fit a model, get predictions, and access the alpha and coefficient values in the same way you would `RidgeCV()`.\n",
    "\n",
    "In this section, you'll want to call `LassoCV()` using 3-fold cross validation with a random state of 0, train the model on the training data for the **full** dataset (i.e. `X_all_train, y_all_train`), get predictions for the test data, and find the mean-squared-error on your cross-validated ridge model on the test data. Save the mean squared error to variable `lassocv_mse`.\n",
    "\n",
    "You may also want to print the optimal alpha value to make sure it's consistent with your plot in question 3.3. Note that it's not unusual if LassoCV() returns an optimal alpha that's slightly different than the alpha that shows up at the minimum point in your plot in question 3.3, due to how LassoCV() solves for coefficients. However, the alpha value should be within a reasonable range of the minimum alpha you get in question 3.3.\n",
    "\n",
    "Again, you can use the same range of alphas that you did for plotting, or you can choose to use a smaller, larger, or more granular range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "kf = KFold(...) # get KFold cross-validation selector object\n",
    "lassocv = LassoCV(cv = ..., alphas=...) # pass CV object and list of alphas to LassoCV()\n",
    "lassocv.fit(...) # fit RidgeCV model on training data\n",
    "\n",
    "alpha_opt = ... # get optimal alpha value\n",
    "print(\"optimal alpha:\", alpha_opt)\n",
    "\n",
    "y_pred_lassocv = ... # get test predictions using RidgeCV model\n",
    "\n",
    "lassocv_mse = ... # get MSE of LassoCV model\n",
    "print(\"Test MSE with cross-validated Lasso:\", lassocv_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside: at this point, you might be asking yourself why we wrote the function in Question 3.1 when we can use `RidgeCV()` and `LassoCV()`. While `RidgeCV()` and `LassoCV()` are very useful for getting optimal alphas and corresponding coefficients and mean squared errors, it's harder to extract the cross-validated mean squared error for a *list* of alphas from those functions (particularly from `LassoCV()`). We wanted you to have both tools (a self-written function that calculates MSE across alphas, and scikit-learn provided functions that return the optimal alpha and coefficients) at your disposal.\n",
    "\n",
    "Moving on!\n",
    "\n",
    "**Question 4.3** Now, let's see how Novotny's model performed. The list `X_nov` below are the predictors that they put in their final model data set.  Use that list to fit a multiple linear regression model whose predictors is just the predictors listed in `X_nov`.  We're going to fit Novotny's model to our training data and test it on our testing data.\n",
    "\n",
    "That means we'll fit the linear regresion model to the training dataset we've been using throughout the lab (`X_all_train, y_all_train`) but select only the columns in `X_nov` from that training set. Then calculate the mean squared error between the model's prediction on the test data set (i.e. `X_test`, but containing only the columns in `X_nov`) and the actual data for the test set.  Save the MSE value to `novotny_mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block\n",
    "X_nov = ['WRF+DOMINO',\n",
    "       'Impervious_800', 'Elevation_truncated_km', 'Major_800',\n",
    "       'Resident_100', 'Distance_to_coast_km']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student version\n",
    "X_nov_train = \n",
    "X_nov_test = \n",
    "\n",
    "...\n",
    "\n",
    "print(\"Test MSE with Novotny model:\", novotny_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.4** Which model performs best?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.5** In this question, we compared the MSE of 3 models. Name and describe one other metric we could use to compare models. How do you think the relative model performance of ridge, lasso, and Novotny's model would differ using that metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Project<a name='proj'></a>\n",
    "\n",
    "For the project-related questions in this homework, we're going to step back and regroup! We know that projects have evolved since you first put down your ideas in HW4. In this section, we're going to re-ask some questions from earlier to get a sense of where you're at now with the project, and then we'll ask a question to get you thinking about the division of work in your group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.1** What prediction questions will your group be exploring? Ultimately, we want you to ask one question per student in your group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.2** What data sources will you be using? You should aim for 1 + $N_s$ data sources, where $N_s$ is the number of students in your group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.3** So far, what has everyone's contribution been to the project? Everyone in your group should answer this question individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.4** How do you envision dividing the workload of the project among your group members? Try to think through all the different phases of the project, from cleaning data to modeling and analysis and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Submission\n",
    "\n",
    "Congrats, you're done with homework 8!\n",
    "\n",
    "Before you submit, click **Kernel** --> **Restart & Clear Output**. Then, click **Cell** --> **Run All**. Then, go to the toolbar and click **File** -> **Download as** -> **.html** and submit the file **as both an .html and .ipynb file through bCourses**.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Alex Nakagawa\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
