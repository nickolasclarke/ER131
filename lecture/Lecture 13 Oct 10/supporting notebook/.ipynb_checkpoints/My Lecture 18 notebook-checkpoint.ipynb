{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 18 in-class notebook\n",
    "\n",
    "Duncan Callaway\n",
    "\n",
    "October 23 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction.\n",
    "This notebook will take us through some of the basics of cross-validation.  Along the way we'll learn a little about how to *resample* data.  \n",
    "\n",
    "And from an applied perspective, we'll explore the differences between indoor and outdoor PM concentrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this directory are five data files.  These are taken from PM sensors in the East Bay.  Two are indoor air sensors:\n",
    "- parkside\n",
    "- kirchy\n",
    "\n",
    "The three others are outdoor sensors:\n",
    "\n",
    "- westoak\n",
    "- hilltop\n",
    "- blaster\n",
    "\n",
    "Here's a map:\n",
    "![alt text](sensor_map.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data.\n",
    "Let's create a list of strings that name each sensor -- this will help automate the loading and processing of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = ['parkside', 'westoak', 'blaster', 'hilltop', 'kirchy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next loop will go through each of the strings in `places` and \n",
    "- load the data, \n",
    "- create a datetime vector from the time *string* in the csv files (this takes a while computationally)\n",
    "- filter out data points before Sept 23rd (this is when I installed my sensor)\n",
    "- create a Series from the PM2.5 data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(places)):\n",
    "\n",
    "    # load the data:\n",
    "    df = pd.read_csv(places[i] + '.csv')\n",
    "    \n",
    "    # create a datetime column\n",
    "    df['time'] = pd.to_datetime(df['created_at'], utc=True)\n",
    "    \n",
    "    # filter out early measurements\n",
    "    bool = df['time'] > '2018-09-23'\n",
    "    df = df.loc[bool,:]\n",
    "    \n",
    "    # create the series\n",
    "    df_series = pd.Series(df.loc[:,'PM2.5_CF_ATM_ug/m3'])\n",
    "    df_series.index = df['time']\n",
    "    \n",
    "    # The `exec` function below allows me to create variables from the strings in `places`\n",
    "    exec (places[i] + \"= df_series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the tops of these new series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kirchy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hilltop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Questions**\n",
    "\n",
    "1. What's the granularity of the data in time?  \n",
    "2. What problems will we run into if we fit models relating one measurement to the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Answers:**\n",
    "\n",
    "1. About once per minute, but not exactly\n",
    "2. The measurements are not coincident. Therefore we can't faithfully relate one to the other.\n",
    "\n",
    "Now, note that we don't know that the clocks of these sensors are synchronized -- but they are internet-connected, so we're going to assume they're close.\n",
    "\n",
    "To address this we'll use pandas' `resample` method, which aggregates data situated in time into blocks of time of fixed length.  In our case we'll aggregate by averaging all data in a particular window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = '5T' # this will create a five minute averaging window\n",
    "hilltop_5T = hilltop.resample(agg).mean()\n",
    "hilltop_5T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see that the time stamps are very regular.  \n",
    "\n",
    "Note the default is to put all data in the 0-5 minute window in the measurement marked 0, 5-10 minute window into the 5 minute measurement, and so on.\n",
    "\n",
    "There are a number of other things one can customize in the execution of the function.  Read the documentation...\n",
    "\n",
    "Now let's apply this to all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = '30T' # number of minutes into which to average data. Note that I experimented with smaller averaging windows and found some data to be missing\n",
    "\n",
    "# in what follows, bfill replaces any missing data with the data from the following interval.  \n",
    "for i in range(len(places)):\n",
    "    exec (places[i] + \"_resamp = \" + places[i] + \".resample(agg).mean().bfill()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That just created five series that are \"resampled\", but let's try to call it \"aggregated\" since we're referring to resampling as the process of creating new data sets from your full data set.\n",
    "\n",
    "In this case, all the data in each 30 minute window are averaged to a single value representing the entire 30 minutes.\n",
    "\n",
    "There are missing data -- so I used the `.bfill()` method.  This way, times when data are missing are given the data point for the next time step.  \n",
    "\n",
    "For each series, no more than 2 values get backfilled. I chose 30T as the time window in part to reduce the number of backfill operations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "Now let's build the KNN model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "k = 25 # note, lower case will be number of neighbors, upper case (later) will be number of folds.\n",
    "neigh = KNeighborsRegressor(n_neighbors=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define X and y variables.  Note that we need to `reshape` the X variable to put it in the shape that sklearn asks for.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(hilltop_resamp).reshape(-1,1)\n",
    "y = np.array(kirchy_resamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and fit the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh.fit(X, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `neigh` contains the KNN estimate of the y-data (kirchy, an indoor sensor) from the X-data (hilltop, an outdoor sensor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions with the model\n",
    "Let's create a vector of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = np.arange(0,100).reshape(-1,1)\n",
    "yhat = neigh.predict(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and let's plot what we've created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y)\n",
    "plt.plot(X_pred,yhat, 'r-')\n",
    "\n",
    "plt.xlim([0,100])\n",
    "plt.ylim([0,100])\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that we are probably over-fitting the data to the model.  K is too small, most likely.  \n",
    "\n",
    "Now, going forward we need a way to cross-validate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning the data for cross validation\n",
    "First, we need to create partitions of data\n",
    "\n",
    "If we just partitioned the data into $K$ folds in the order they occur, we might inadvertently pick up correlation between variables in their order of occurrence.  \n",
    "\n",
    "For example suppose you're predicting temperature for tomorrow based on today's temperatures.  If you sample just from the morning then you'll get a bias downward since temperatures are typically lower then.  \n",
    "\n",
    "But it's also important to use disjoint folds -- that is, no two folds should contain the same data.  Why?  Because we want to make sure that when we reserve a fold for testing data those data won't also appear in the training set.\n",
    "\n",
    "But how can we do this?  \n",
    "- draw random numbers without replacement\n",
    "- randomly \"shuffle\" the data, then make the first block (roughly $n/K$ data points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a shuffle.  Numpy has a built in command for that.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter code here\n",
    "x = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run that over and over you'll see the order of the second array changes.  You can stop that with a seed, e.g. `np.random.seed(3)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do next?  We need to partition the data.  If we want $K$ folds, then we'll need to create $K$ partitions.  Let's try K = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4\n",
    "d1 = int(np.ceil(len(x)/K))\n",
    "d2 = K\n",
    "x_partitioned = np.full((d1, d2), np.nan)\n",
    "\n",
    "for i in range(K):\n",
    "    x_partitioned[0:len(x[i::K]),i] = x[i::K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_partitioned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see we have an array with randomly placed entries from the original series.  Each of these can be thought of as a fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlining K-fold with scikit-learn:\n",
    "Now let's think about how to fit models and construct errors for each.\n",
    "\n",
    "Because we're going to be splitting both the input and response variables, I'm going to transition now to using a built-in class from scikit-learn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as skm\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = skm.KFold(n_splits=K, shuffle=True, random_state=7) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That makes an object that can put data into K splits.  \n",
    "\n",
    "Let's redefine $x$ so it's ordered.  This will allow us to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter code here\n",
    "x = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter more code here \n",
    "for ...:\n",
    "    print(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if the number of data points is not divisible by $K$, KFold actually makes a number of folds that are one data point less in length, rather than one split that is a number of data points shorter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K = 10\n",
    "kf = skm.KFold(n_splits=K, shuffle=True, random_state=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(hilltop_resamp).reshape(-1,1)\n",
    "y = np.array(kirchy_resamp)\n",
    "\n",
    "# set parameters\n",
    "k_min = 10\n",
    "k_max = 50\n",
    "step = 10\n",
    "\n",
    "# initialize k\n",
    "k_vect = np.arange(k_min, k_max, step)\n",
    "n_steps = len(k_vect)\n",
    "\n",
    "# initialize MSEs\n",
    "MSE_test = np.zeros(n_steps)\n",
    "MSE_train = np.zeros(n_steps)\n",
    "\n",
    "#loop on number of neighbors.  Inner loop takes us through each split of the data.\n",
    "for i in range(len(k_vect)):\n",
    "    MSE_test[i] = 0\n",
    "    MSE_train[i] = 0\n",
    "    neigh = KNeighborsRegressor(n_neighbors=k_vect[i])\n",
    "    for train, test in kf.split(X):  # test and train will be indices we can use to partition the data\n",
    "        neigh.fit(X[train], y[train]) \n",
    "        MSE_test[i] = MSE_test[i]+ 1/K * np.mean((neigh.predict(X[test]) - y[test])**2)\n",
    "        MSE_train[i] = MSE_train[i]+ 1/K * np.mean((neigh.predict(X[train]) - y[train])**2)\n",
    "        \n",
    "test_train = np.array([MSE_test, MSE_train]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_vect, test_train)\n",
    "plt.xlabel('$k$ for K-nearest neigbor model')\n",
    "plt.title('MSE versus number of neighbors')\n",
    "plt.ylabel('mean squared error');\n",
    "plt.legend(['test MSE', 'train MSE']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the final model\n",
    "Now that we know the right flexibility for the model (50 neighbors), we can build the final model with *all* the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_opt = k_vect[MSE_test == np.min(MSE_test)][0]\n",
    "k_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_knn = KNeighborsRegressor(n_neighbors=k_opt)\n",
    "final_knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How well does outdoor air quality predict indoor air quality?\n",
    "Now let's come back to the question: how well does outdoor air quality predict indoor air quality?\n",
    "\n",
    "Let's do the following\n",
    "1. Build a model that predicts outdoor air at one location from outdoor air at another.\n",
    "2. Build a model that predicts indoor air at one location from outdoor air at another.\n",
    "3. Compare performance of the two.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(hilltop_resamp).reshape(-1,1)\n",
    "y_hilltop = np.array(kirchy_resamp)\n",
    "\n",
    "final_knn = KNeighborsRegressor(n_neighbors=k_opt)\n",
    "final_knn.fit(X, y_hilltop)\n",
    "\n",
    "kirchy_from_hilltop = final_knn \n",
    "kirchy_pred = kirchy_from_hilltop.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_westoak = np.array(westoak_resamp)\n",
    "\n",
    "final_knn = KNeighborsRegressor(n_neighbors=k_opt)\n",
    "final_knn.fit(X, y_westoak)\n",
    "\n",
    "westoak_from_hilltop = final_knn\n",
    "westoak_pred = westoak_from_hilltop.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(6,6))\n",
    "ax1 = f.add_subplot(211)\n",
    "\n",
    "\n",
    "# first subplot -- vs indoor\n",
    "plt.scatter(X,y_hilltop)\n",
    "plt.plot(X_pred, kirchy_pred, 'r-')\n",
    "plt.plot(X_pred, X_pred, 'y')\n",
    "\n",
    "plt.xlim([0,100])\n",
    "plt.ylim([0,100])\n",
    "\n",
    "plt.xlabel('Hilltop Oakland (outdoor)')\n",
    "plt.ylabel('Kirchy (indoor)');\n",
    "plt.title('Indoor versus outdoor PM concentrations');\n",
    "\n",
    "plt.legend([ 'KNN model', 'data', 'y=x'])\n",
    "# second subplot -- vs outdoor\n",
    "ax2 = f.add_subplot(212)\n",
    "\n",
    "plt.scatter(X,y_westoak)\n",
    "plt.plot(X_pred, westoak_pred, 'r-')\n",
    "plt.plot(X_pred, X_pred, 'y')\n",
    "\n",
    "plt.xlim([0,100])\n",
    "plt.ylim([0,100])\n",
    "\n",
    "plt.ylabel('West Oakland (outdoor)')\n",
    "plt.xlabel('Hilltop (outdoor)');\n",
    "\n",
    "plt.title('Outdoor versus outdoor PM concentrations');\n",
    "\n",
    "# clean up sizing\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we observe from the plots?\n",
    "1. Indoor appears to be *usually* lower than Hilltop outdoor\n",
    "2. West Oakland outdoor is on average higher than Hilltop outdoor\n",
    "3. But the indoor extremes are really extreme -- pull the estimates up.\n",
    "4. When the data \"run out\" the model is systematically high (high values of outdoor predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
