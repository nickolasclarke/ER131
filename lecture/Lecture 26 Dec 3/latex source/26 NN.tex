
\documentclass[mathserif, aspectratio=169]{beamer}

\usepackage{psfrag,graphicx}
\usepackage{amsmath}
\usepackage[absolute,overlay]{textpos}

\usepackage{braket}
\graphicspath{{figs/}}

\usetheme{Boadilla}
\makeatother
\setbeamertemplate{footline}[frame number]

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage{amsthm}  
\usepackage{bm}
\usepackage{lipsum}
\usepackage[linesnumbered, ruled]{algorithm2e}
\usepackage{color}
\newtheorem{assumption}{Assumptions}
\newtheorem{prop}{Proposition}
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{sol}{Decentralized Solution}
\newtheorem{thresh}{$\epsilon$-thresholding}
\definecolor{light-gray}{gray}{0.8}
\usepackage{textcomp}

\newcommand{\backupbegin}{
   \newcounter{finalframe}
   \setcounter{finalframe}{\value{framenumber}}
}
\newcommand{\backupend}{
   \setcounter{framenumber}{\value{finalframe}}
}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

\makeatletter
\setbeamertemplate{navigation symbols}{}

\title[Lecture 26] 
{Data, Environment and Society: \\{Lecture 26: Neural Networks}}



\author[ER131: Data, Environment and Society] 
{Instructor: Duncan Callaway\\
GSI: Salma Elmallah} 

%\logo{
%\includegraphics[width=1.5cm,height=1.5cm,keepaspectratio]{uvic_logo_h.jpg}
%}
\vspace{-20mm}
\institute[UC Berkeley] % (optional, but mostly needed)
 {\small{ \bf December 3, 2019}}

\date[December 3, 2019]

\begin{document}

\frame{
	\titlepage
}

\begin{frame}{Annoucements}

\begin{itemize}
	\item Thursday: Career panel!  
	\item Poster presentation: 3-6pm, Dec 17, Kvamme Atrium in Sutardja Dai Hall 3rd floor.
	\begin{itemize}
		\item Poster instructions in GitHub, \href{https://github.com/duncancallaway/ER131_2019/tree/master/poster}{here}.
	\end{itemize}
	\item Project due Dec 18 6a.  
\end{itemize}

\end{frame}

\begin{frame}{Last time}
    \begin{columns}
    	\column{0.5\textwidth}
    		In words, each neuron...
    		\begin{itemize}
    			\item Takes a vector of values as inputs
    			\item Creates a scalar from a linear combination of the vector entries
    			\item Passes the resulting scalar through an ``activation function''
    			\item Outputs a single value from that activation function
    		\end{itemize}

    		Terminology analogies:
    		\begin{itemize}
    			\item Electrical signal from other cells $\rightleftarrows$ input
    			\item Neuron $\rightleftarrows$ Activation function
    			\item Electrical signal to other cells $\rightleftarrows$ output
    		\end{itemize}
    	\column{0.5\textwidth}
    	\pause
    	\includegraphics[width=0.85\textwidth]{neuron_math.pdf}
	    	
    \end{columns}

\end{frame}

\begin{frame}{Glue neurons together, backpropogate, and off you go!}
    
\includegraphics[width = \textwidth]{tensorflow.pdf}

\end{frame}

\begin{frame}{Today}
    
Let's just talk about a few important and fun applications of neural networks.  

\end{frame}

\begin{frame}{Example resource allocation with NN}
\begin{figure}
\includegraphics[width=0.7\textwidth]{jean_header}
\caption*{}
\end{figure}
\end{frame}

\begin{frame}{The challenge}

\begin{itemize}
\item International aid agencies want information on where to spend effort
\item Knowing where the poorest people are can inform these decisions
\item But surveying a country's population is expensive, and most survey results are not in the public domain.  
\begin{itemize}
\item Some African countries do not run \textit{any} surveys on wealth and poverty. 
\end{itemize}
\item Question: can one train a model that uses remote sensing data to \textit{predict} poverty?
\end{itemize}

\end{frame}

\begin{frame}{Night lights}

	\begin{itemize}
		\item Some folks have used night satellite imagery to estimate spatial distributions of wealth
			\begin{itemize}
				\item Basic idea: satellites can see lighting activity at night; People with more money use more night lighting.  
			\end{itemize}
		\item These methods are not accurate at extreme poverty income levels ($<\$1.90$ per person per day)
		\item There is little data on extreme poverty (see previous slide) so there isn't a large data set to train models with.
	\end{itemize}
	\begin{center}
		\includegraphics[width=0.6\textwidth]{earth_lights_lrg}
	\end{center}
\end{frame}

\begin{frame}{Jean \textit{et al's} idea}
\begin{enumerate}
\item Use \textit{daytime }satellite imagery, not night time
\begin{itemize}
\item Landcover type and structures, for example, ought to help to predict poverty.
\end{itemize}
\item Deal with the data paucity problem via ``transfer learning''
\begin{itemize}
\item The idea here is pretty simple: train a neural net on a well known set of images (ImageNet -- labelled data from 1,000 categories, e.g. ``boneshaker'', ``crutch'', ``miniature schnauzer'')

\includegraphics[height=0.3\textheight]{schnauzer} \;\includegraphics[height=0.3\textheight]{boneshaker}\;\includegraphics[height=0.3\textheight]{crutch.jpg}

\item its value isn't so much in finding schnauzers, but in being good at finding the low level features (edges, corners) common to many vision tasks
\end{itemize}
\end{enumerate}
\end{frame}

\begin{frame}{A bit more on Jean \textit{et al's} transfer learning}
\begin{enumerate}
 \setcounter{enumi}{2}
\item ImageNet-trained model then ``fine tuned'' to use daytime satellite imagery inputs to produce features that predict nighttime light intensities outputs.
\begin{itemize}
\item In doing this Jean\textit{ et al} are falling back on the idea of using night lights (or a prediction of night lights) to measure income and wealth
\item It's desirable to use night lights because it's a globally available data set and is a better proxy for economic activity than the daytime images would be.
\end{itemize}
\item Then build a ridge regression model:
\begin{itemize}
	\item Target: mean cluster-level values from \textit{survey} data (where available) 
	\begin{itemize}
		\item Clusters are geographic areas approximately the size of a village.
	\end{itemize}
	\item Features: the corresponding image features extracted from the fine tuned ImageNet-trained model  
\end{itemize}
\end{enumerate}
\end{frame}

\begin{frame}{Basic summary of process}

\begin{itemize}
\item Old: night lights $\rightarrow $ predict wealth and income
\item New: daytime imagery $\rightarrow$ predict night lights $\rightarrow$ predict wealth and income.
\end{itemize}

\end{frame}

\begin{frame}{Wait -- I thought night lights were a lousy proxy for economic activity at low income levels?}

\begin{itemize}
\item Jean \textit{et al} address (or try to address) this head on.  
\item Their claim is that because they're using a linear model (ridge regression) to map day time imagery to  night lights, the model is going to be driven by light-economic relationships at higher income levels.  
\begin{itemize}
 \item If they're lucky, the lower income relationship between \textit{estimated} night lights and economic activity will be decent.
 \end{itemize} 
\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Results}
	\begin{columns}[c]
		\column{0.5\textwidth}
			\includegraphics[width=1\textwidth]{jean_fig3}
		\column{0.5\textwidth}
			\includegraphics[width=1.1\textwidth]{jean_figS3}
	\end{columns}
\end{frame}

\begin{frame}{External Validity}

\includegraphics[height = \textheight]{cross-border}

\end{frame}


\begin{frame}{Another neural network story: Google Translate}

Early in November, 2016, Google Translate produced the following:\\

\vspace{5mm}
\begin{columns}
	\column{0.25\textwidth}

	\column{0.5\textwidth}

	``Kilimanjaro is 19,710 feet of the mountain covered with snow, and it is said that the highest mountain in Africa. Top of the west, “Ngaje Ngai” in the Maasai language, has been referred to as the house of God. The top close to the west, there is a dry, frozen carcass of a leopard. Whether the leopard had what the demand at that altitude, there is no that nobody explained.''

	\column{0.25\textwidth}
	\end{columns}

\vspace{5mm}

This is the translation of University of Tokyo Professor Jun Rekimoto's Japanese translation of the first paragraph of Hemingway's ``The Snows of Kilimanjaro''
\end{frame}

\begin{frame}{But something happened overnight:}

One of these is Hemingway's original.  One is the Google Translate version.  Which is which?

\vspace{10mm}

\begin{columns}
	\column{0.5\textwidth}
	``Kilimanjaro is a snow-covered mountain 19,710 feet high, and is said to be the highest mountain in Africa. Its western summit is called the Masai `Ngaje Ngai,'' the House of God. Close to the western summit there is the dried and frozen carcass of a leopard. No one has explained what the leopard was seeking at that altitude.''


	\column{0.5\textwidth}
	``Kilimanjaro is a mountain of 19,710 feet covered with snow and is said to be the highest mountain in Africa. The summit of the west is called `Ngaje Ngai' in Masai, the house of God. Near the top of the west there is a dry and frozen dead body of leopard. No one has ever explained what leopard wanted at that altitude.''
\end{columns}

\pause
\vspace{10mm}
Are neural networks becoming more like humans?  Or just better at imitating them?
\begin{itemize}
	\item In this case, Google moved to a NN engine that translates whole sentences rather than phrases.  In effect, it's just solving a much bigger classification problem than before.  
\end{itemize}
\end{frame}


\begin{frame}{A side note on using models trained on one data set to interpret another}

\pause
% ask class: what do they remember seeing in the lower levels of the tensorflow playground?

	Image classification:
	\begin{itemize}
		\item Each layer of NN extracts higher and higher-level features of the image
		\item The final layer makes a decision on what the image shows. 
		\item Example: 
		\begin{itemize}
			\item First layer might look for edges or corners. 
			\item Intermediate layers interpret basic features to identify components (wheel, cliff...)
			\item Final layers assemble those into complete interpretations (bicycle, mountain range...)
		\end{itemize}
		\item Google researchers played around with this:
		\begin{itemize}
			\item Starting with an NN trained on ImageNet
			\item Train the NN predict -- or interpret -- parts of new images
			\item Look at intermediate layers of the interpretation
		\end{itemize}
	\end{itemize}

	More information \href{https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html}{\textbf{here}}; let's look at a few of their images now.

\end{frame}

\begin{frame}{Example: Low layer interpretation}
Here we see accentuation of strokes and colors.
\begin{figure}
\includegraphics[width=\textwidth]{ibis_google}
\caption*{}
\end{figure}
\end{frame}

\begin{frame}{Example: High layer interpretation}
Here we see accentuation of specific kinds of features
\begin{figure}
\includegraphics[width=\textwidth]{skyarrow_google}
\caption*{}
\end{figure}
\end{frame}

\begin{frame}{A zoom in on the cloud images}
\begin{figure}
\includegraphics[width=\textwidth]{funny-animals_google}
\caption*{}
\end{figure}

\vspace{-10mm}
\pause
Are these the dreams of neural networks?
\begin{itemize}
	\item Reporters that covered the research liked to spin it that way
	\item I tend to think of this more pragmatically -- these are just complex mathematical interpretations of images.
	\item BUT: I do think the images are beautiful and related to the way humans play with images
	\item This certainly pushes computing in the direction of art and spirituality, which I find scary and inspiring
\end{itemize}
\end{frame}

\begin{frame}[t]\frametitle{}
	\textbf{Course wrapup: What do I hope you've learned?}
\end{frame}

\begin{frame}
	\frametitle{Python}
	\includegraphics[height=\textheight]{python}
\end{frame}

\begin{frame}
	\frametitle{Visualization}
	\begin{center}
		\includegraphics[height=0.9\textheight]{pop-income}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{Resampling}
	\begin{center}
		\includegraphics[height=0.65\textheight]{bootstrap}
		\includegraphics[height=0.5\textheight]{loocv}
	\end{center}
\end{frame}

\begin{frame}
	\frametitle{Model identification}
	\begin{itemize}
		\item Parameters fit by loss minimization
		\begin{itemize}
			\item These determine the shape of the function you're fitting
		\end{itemize}
		\item Hyperparameters fit by iteration and cross validation
		\begin{itemize}
			\item These enable you to tune bias-variance tradeoff
		\end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}{Zoom out: Train, (cross) validate and test}
	Using slightly different language, some definitions from from Brian Ripley, Pattern Recognition and Neural Networks, 1996, page 354:
	\vspace{5mm}
	\begin{itemize} 
		\item \textbf{Training set}: ``A set of examples used for learning, that is to fit the parameters of the classifier.''
		\begin{itemize}
			\item{For minimizing the loss function}
		\end{itemize}
		\item\textbf{ Validation set}: ``A set of examples used to tune the parameters of a classifier, for example to choose the number of hidden units in a neural network.''
		\begin{itemize}
			\item For choosing hyperparameters.
		\end{itemize}
		\item \textbf{Test set}: ``A set of examples used only to assess the performance of a fully-specified classifier.''
		\begin{itemize}
			\item For a final check -- no more model fitting allowed here!
		\end{itemize}
	\end{itemize}
	\vspace{5mm}
\end{frame}



\begin{frame}
	\frametitle{New tools for prediction: regression and classification}
	\begin{columns}[c]
		\column{0.7\textwidth}
			\begin{itemize}		
				\item Linear regression models: Ridge, Lasso, Elastic net
				\begin{align*}
					\min_\beta \sum_{i=1}^N \left(Y_i - X_i \beta \right)^2+\lambda \cdot R(\beta)
				\end{align*}
				\item Decision trees
				\item Support vector machines
			\end{itemize}
		\column{0.25\textwidth}
			\includegraphics[width=\textwidth]{tree_vs_linear_for_classification}

			\includegraphics[width=\textwidth]{SVM_example}
	\end{columns}
\end{frame}

\begin{frame}
	\frametitle{Inference, prediction and resource allocation in environmental contexts}
	\begin{columns}[c]
		\column{0.3\textwidth}
			\includegraphics[width = \textwidth]{marshall_injustice}
		\column{0.7\textwidth}
			\begin{itemize}
				\item Prediction in space: what's happening in places where I have limited data?
				\begin{itemize}
					\item Example: What are the NO2 concentrations where we don't have monitoring equipment?
					\item Example: What are the land cover characteristics in a region?
				\end{itemize}
				\item Prediction across communities: What's happening in communities where I have limited information?  
				\begin{itemize}
					\item Example: What communities in California have clean water?
				\end{itemize}
				\item Prediction in time: What will happen next year, or tomorrow?  
				\begin{itemize}
					\item Example: will the air be clean tomorrow?
				\end{itemize}
			\end{itemize}
	\end{columns}
\end{frame}

\begin{frame}
	\frametitle{Next steps: What I hope for you}
	\begin{itemize}
		\item You've now got basic skills for data manipulation and modeling and working in Python
		\item You've also got experience developing research questions and defining resource allocation problems
		\item You should feel proud of these skills when you present yourself to potential research advisers and employers
		\begin{itemize}
			\item Environmental and Justice nonprofits and NGOs
			\item Government regulators
			\item Energy sector companies
		\end{itemize}
		...all these folks are looking for the skills you now have
		\item You're also ready to take more courses in the area: DS100, CS189, Stat 154.
		\item I also hope you'll keep in touch -- tell me how you use these skills!
	\end{itemize}
\end{frame}


\begin{frame}
\center
	\href{https://course-evaluations.berkeley.edu/}{https://course-evaluations.berkeley.edu/}
\end{frame}
\end{document}
