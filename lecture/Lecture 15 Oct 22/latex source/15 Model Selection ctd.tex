%\documentclass[mathserif, aspectratio=169]{beamer}
\documentclass[mathserif, handout, aspectratio=169]{beamer}


\usepackage{movie15}
\usepackage{psfrag,graphicx}
\usepackage{amsmath}
\graphicspath{{figs/}}

\usetheme{Boadilla}
\makeatother
\setbeamertemplate{footline}[frame number]

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage{amsthm}  
\usepackage{bm}
\usepackage{lipsum}
\usepackage[linesnumbered, ruled]{algorithm2e}
\usepackage{color}
\newtheorem{assumption}{Assumptions}
\newtheorem{prop}{Proposition}
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{sol}{Decentralized Solution}
\newtheorem{thresh}{$\epsilon$-thresholding}
\definecolor{light-gray}{gray}{0.8}
\usepackage{textcomp}

\newcommand{\backupbegin}{
   \newcounter{finalframe}
   \setcounter{finalframe}{\value{framenumber}}
}
\newcommand{\backupend}{
   \setcounter{framenumber}{\value{finalframe}}
}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

\makeatletter
\setbeamertemplate{navigation symbols}{}


\title[Lecture 15] % (optional, use only with long paper titles)
{Data, Environment and Society: \\{Lecture 15: Model Selection and Regularization, continued}}


%\subtitle
%{Include Only If Paper Has a Subtitle}

\author[ER190C: Data, Environment and Society] 
{Instructor: Duncan Callaway\\
GSI: Salma Elmallah} 
% - Give the names in the same order as the appear in the paper.
% - Use the \inst{?} command only if the authors have different
%   affiliation.

%\logo{
%\includegraphics[width=1.5cm,height=1.5cm,keepaspectratio]{uvic_logo_h.jpg}
%}
\vspace{-20mm}
\institute[UC Berkeley] % (optional, but mostly needed)
 {\small{ \bf October 22, 2019}}


\date[October 22, 2019]

\begin{document}

\frame{
  \titlepage
}

\begin{frame}{Plagiarism}

``Plagiarism is defined as use of intellectual material produced by another person without acknowledging its source, for example:
 
\begin{itemize}
	\item \textbf{Wholesale copying of passages from works of others into your homework, essay, term paper, or dissertation without acknowledgment.}
	\item Use of the views, opinions, or insights of another without acknowledgment.
	\item Paraphrasing of another personâ€™s characteristic or original phraseology, metaphor, or other literary device without acknowledgment.''
\end{itemize}
 (source: Berkeley Division of Student Affairs)
\end{frame}

\begin{frame}{Recap lecture objectives from last time}
\begin{itemize}
\item Refine our understanding of model identification as an optimization problem
\uncover<2->{\begin{align*}
\min_\beta \sum_{i=1}^N \left(Y_i - X_i \beta \right)^2+\lambda \cdot R(\beta)\end{align*}}
\uncover<3->{Important: We drop the $\lambda$ term for prediction, i.e. predictions are just
\begin{align*}
\hat{y_i} =  X_i \hat{\beta}
\end{align*}
where $X_i$ and $\hat{\beta}$ are vectors}
\item<4-> Understand what ``regularization'' is and why we do it
\begin{itemize}
\item<5-> A tool for adapting optimization problems to be ``well behaved''
\item<5-> In statistical learning, a tool to tradeoff bias and variance
\end{itemize}
\uncover<5->{But note, $R$ causes you to solve a different problem than the original $\rightarrow$ parameter bias}

\end{itemize}
\end{frame}


\begin{frame}{Recap lecture objectives from last time, ctd}
\begin{itemize}
\item Continue thinking about how to adjust errors to compare models with different $p$
\begin{itemize}
\item<2-> k-fold cross validation, AIC, BIC, adjusted R$^2$...
\end{itemize}

\vspace{5mm}
\item Learn the tradeoffs between subset selection, ridge and lasso
\begin{itemize}
\item<3-> Speed (fastest to slowest): Ridge, Lasso, Subset
\item<3-> Subset selection and Lasso do feature selection.  Ridge does not.
\item<3-> You can naturally tune prediction bias-variance with Ridge and Lasso
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Today's objectives}

\begin{enumerate}
\item Quick review of the basic mechanics of Subset selection, Ridge and Lasso.
\item Build deeper intuition on how they work and how they differ.
\item Learn how the bias-variance tradeoff gets tuned with regularization term parameters. 
\item Understand the tradeoffs between these methods in more detail
\item Understand the importance of standardizing your variables.
\item Epilogue: the elastic net, a machine learning mashup.  
\end{enumerate}

\end{frame}

\begin{frame}{Objective 1}

Some quick review.

\end{frame}

\begin{frame}{Stepwise selection: Forward Selection}

\begin{columns}
\column{0.5\textwidth}

\textbf{Forward selection:} Start with $\mathcal{M}_0$.  Then to choose the best model from each higher ``level'':

\vspace{5mm}
\textit{First, }within each level, add one predictor at a time to the best model from the lower level.  Use $R^2$ or other to find the best model from this set of $\mathcal{M}_{k-1} \text{``}+ 1\text{''}$

\vspace{5mm}

\textit{Second,} choose from your list $\{\mathcal{M}_0, \mathcal{M}_1,...\mathcal{M}_{p}\}$ via cross validation or adjusted error metrics.\\~\\

\column{0.5\textwidth}

\vspace*{-15mm}
\begin{center}
\only<1>{\includegraphics[scale=0.425]{4choosek}}
\only<2>{\includegraphics[scale=0.425]{4choosek_M0}}
\only<3>{\includegraphics[scale=0.425]{4choosek_M1}}
\only<4>{\includegraphics[scale=0.425]{4choosek_M2}}
\only<5>{\includegraphics[scale=0.425]{4choosek_M3}}
\only<6>{\includegraphics[scale=0.425]{4choosek_M4}}
\end{center}
\end{columns}

\end{frame}


\begin{frame}{A bit more recap}

\begin{align*}
\min_\beta \sum_{i=1}^N \left(Y_i - X_i \beta \right)^2+\lambda \cdot R(\beta)\end{align*}

The regularizing term, $R(\beta)$ can take a lot of forms.  We looked at
\begin{align*}
R(\beta) & =\sum_{i=1}^K I(\beta_i) &= \norm{\beta}_0 & \quad \text{(subset selection)} &\\
R(\beta) & =\sum_{i=1}^K |\beta_i| & = \norm{\beta}_1 & \quad \text{(lasso)} &\\
R(\beta) & =\sum_{i=1}^K \beta_i^2 & = \norm{\beta}_2^2 & \quad \text{(ridge)} &
\end{align*}

Note, last time I referred to $\sum_{i=1}^K \beta^2$ as the 2-norm.  It's not!  ($\norm{\beta}_2 = \sqrt{\sum_{k=1}^K \beta^2}$ is.)

\end{frame}


\begin{frame}{Side note:  ``Regularization''}

\textbf{Regularization} Refers to the process of adding a term to the objective function of a problem that 
\begin{itemize}
\item Makes the problem ``well behaved" (easier to solve)
\item In statistical learning, allows us to tune model flexibility
\item Solves a different problem from the one you originally wanted.\\~\\
\end{itemize}

In our case, the sum of squared coefficients in Ridge makes the problem very simple to solve, but we get coefficients that are biased.\\~\\


\end{frame}


\begin{frame}{Objective 2}


Building deeper intuition on how these methods work and how they differ.  
\end{frame}



\begin{frame}{Identifying parameters}

\begin{align*}
\hat{\beta}_\text{ols} = & \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \left(\mathbf{X}^T\mathbf{Y}\right)\\
\hat{\beta}_\text{ridge} =& \left(\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I}_k\right)^{-1} \left(\mathbf{X}^T\mathbf{Y}\right)\\
\hat{\beta}_\text{lasso}  =& \text{ Something you need to solve numerically} \\
&\text{(with something like gradient descent)}
\end{align*}

Here 
\begin{itemize}
\item $\lambda$ is a tuning parameter -- it is not unique.  
\item $\mathbf{I}_k$ is the $k\times k$ identity matrix\\~\\
\end{itemize}
\vspace{-1mm}
\pause
\textbf{Important}! 
\begin{itemize}
	\item Ridge and Lasso produce different $\beta$ estimates for different choices of $\lambda$.
	\item  The $\lambda$ term is not involved in prediction.  Predictions are just $\hat{y_i} =  X_i \hat{\beta}$
\end{itemize}
\end{frame}

\begin{frame}{Regularized problems can be converted to constrained problems}

\begin{align*}
\text{Subset selection:} \quad &\min_\beta \sum_{i=1}^N \left(Y_i - X_i \beta \right)^2+\lambda \cdot \sum_{k=1}^K I(\beta_k)
&\Leftrightarrow
\uncover<2->{\begin{cases}
\min_\beta \sum_{i=1}^N \left(Y_i - X_i \beta \right)^2\\
\text{subject to}\\
\sum_{k=1}^K I(\beta_k) \le s
\end{cases}}
\end{align*}

\pause
Important: 
\begin{itemize}
\item $\lambda$ and $s$ are parameters that need to be tuned. 
\item Increasing $\lambda$ has the same effect as decreasing $s$.  (Forces selection of fewer features.)
\item $\lambda$ and $s$ are not independent.  
\item I'm not deriving this result, but I want the intuition to be clear -- ask questions if needed!
\end{itemize}

\end{frame}


\begin{frame}{Regularized problems can be converted to constrained problems}

\begin{align*}
\text{Subset selection:} \quad &\min_\beta \sum_{i=1}^N \left(Y_i - X_i \beta \right)^2+\lambda \cdot \sum_{k=1}^K I(\beta_k)
&\Leftrightarrow
{\begin{cases}
\min_\beta \sum_{i=1}^N \left(Y_i - X_i \beta \right)^2\\
\text{subject to}\\
\sum_{k=1}^K I(\beta_k) \le s
\end{cases}}\\\\
%
\text{Lasso:} \quad  &\min_\beta \sum_{i=1}^N \left(Y_i - X_i \beta \right)^2+\lambda \cdot \sum_{k=1}^K |\beta_k|
&\Leftrightarrow
\uncover<2->{\begin{cases}
\min_\beta \sum_{i=1}^N \left(Y_i - X_i \beta \right)^2\\
\text{subject to}\\
\sum_{k=1}^K |\beta_k | \le s
\end{cases}}\\\\
%
\text{Ridge:} \quad  &\min_\beta \sum_{i=1}^N \left(Y_i - X_i \beta \right)^2+\lambda \cdot \sum_{k=1}^K \beta_k^2 
&\Leftrightarrow
\uncover<3->{\begin{cases}
\min_\beta \sum_{i=1}^N \left(Y_i - X_i \beta \right)^2\\
\text{subject to}\\
\sum_{k=1}^K \beta_k^2 \le s
\end{cases}}
\end{align*}

\end{frame}

\begin{frame}{Setting intuition about constrained problems}

Imagine a simple two-feature problem.  

\begin{columns}

\column{0.25\textwidth}
\begin{center}
No constraints.  Red lines are ``constant RSS'' contours
\end{center}
\includegraphics[scale=0.65]{RSE_contour}

\pause
\column{0.25\textwidth}
\begin{center}
Subset selection.  If $\sum_{k=1}^K I(\beta_k) \le 1$, solutions in blue area.
\end{center}
\uncover<3->{\includegraphics[scale=0.65]{RSE_subset_constr}}

\column{0.25\textwidth}
\begin{center}
Lasso.   Solutions must be in blue area.
\end{center}
\uncover<4->{\includegraphics[scale=0.65]{RSE_lasso_constr}}

\column{0.25\textwidth}
\begin{center}
Ridge.  Solutions must be in blue area.
\end{center}
\uncover<5->{\includegraphics[scale=0.65]{RSE_ridge_constr}}

\end{columns}

{\tiny Figures adapted from Elements of Statistical Learning}

\end{frame}

\begin{frame}{How do the parameter estimates compare?}
\begin{columns}
\column{0.6\textwidth}
\begin{figure}
\includegraphics[scale=0.45]{lasso-ridge-subset-vs-ols}
\caption*{\tiny Figure taken from Imbens 2015 NBER lecture}
\end{figure}
\column{0.4\textwidth}

Model: $	y_i = \beta x_i + \epsilon_i$

\vspace{5mm}

Lines are the result of increasing the ``true'' $\beta$, then comparing estimates from different loss functions.

\vspace{5mm}

We often call regularization approaches ``shrinkage'' methods because, in general, they smoosh parameters closer to zero.
\end{columns}

\end{frame}


\begin{frame}{Objective 3}
Learn how the bias-variance tradeoff gets tuned with regularization term parameters. 
\end{frame}

\begin{frame}{Lasso or ridge?}

Simulated data set from $n=50$ and $p=45$. Figure shows test MSE on y-axis.  Each point on the curve corresponds to a different $\lambda$.
\begin{columns}
\column{0.5\textwidth}
\begin{figure}
\includegraphics[scale=1]{lasso-v-ridge-45variables}
\end{figure}

\vspace{-10mm}

\begin{center}
The actual response is a function of\\ \textbf{all 45 predictors}.  
\end{center}  

\column{0.5\textwidth}
\uncover<2->{
\begin{figure}
\includegraphics[scale=1]{lasso-v-ridge-2variables}
\end{figure}
\vspace{-10mm}
\begin{center}
The actual response is a function of\\ \textbf{only 2 of the 45 predictors.}
\end{center}  }

\end{columns}

\vspace{5mm}
\footnotesize Red show total MSE.  (Black and green are variance and bias.) 
Dashed line -- best Ridge.  Solid line -- best lasso.  
\end{frame}


\begin{frame}{Lasso or ridge -- a general trend}

Ridge works best when the actual number of features is high.  Lasso works best when the actual number of features is low. 
\vspace{5mm}
\begin{columns}
\column{0.5\textwidth}
\begin{figure}
\includegraphics[scale=1]{lasso-v-ridge-45variables}
\end{figure}
\column{0.5\textwidth}
\begin{figure}
\includegraphics[scale=1]{lasso-v-ridge-2variables}
\end{figure}
\end{columns}

\end{frame}
\begin{frame}{Choosing $\lambda$}

As we've seen, $\lambda$ can take a range of values.  Here we see the tradeoff for the 2-of-45 predictors example  lasso example from the last slide. 


\begin{figure}
\includegraphics[scale=.85]{bias-variance-lasso-2}
\includegraphics[scale=.85]{lasso-tenfold}
\end{figure}

\pause

Simple $\lambda$ selection strategy: use k-fold cross validation to test performance on a grid of $\lambda$ values. 

\vspace{5mm}

Regularize for parameter estimation only!  y-axis errors above are just $\sum_i (y_i-X_i\hat{\beta})^2$

\end{frame}

\begin{frame}{Objective 4}

Understand the tradeoffs between these methods in more detail

\end{frame}


\begin{frame}{Ridge regression advantages over OLS}

\pause

\textbf{First}.  Suppose some of your features are linear combinations of the others.  That means you can write $x_{i,j} = Ax_{i,-j}$ for at least 1 value of $j$. \\~\\

Then $\mathbf{X}^T\mathbf{X}$ is not ``full rank'' and you can't invert it.  I.e., $(\mathbf{X}^T\mathbf{X})^{-1}$ doesn't exist.

\begin{align*}
\text{e.g., }\mathbf{X}^T\mathbf{X} = 
\begin{bmatrix}
1 & 2 & 3\\
2 & 4 & 6\\
3 & 6 & 9
\end{bmatrix}
\uncover<3->{\text{ \quad vs.  \quad}
\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I} =
\begin{bmatrix}
1+\lambda & 2 & 3\\
2 & 4+\lambda & 6\\
3 & 6 & 9+\lambda
\end{bmatrix} }
\end{align*}

\pause

But you \textit{can} invert $\left(\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I}_k\right)$.  (Since you've added a little shift to the diagonals of the matrix, which restores linear independence)\\~\\


\pause

\textbf{Second.}  Computation!  It's faster than subset selection (but solves a different problem if your objective is parameter interpretation).

\end{frame}

\begin{frame}{Ridge regression advantages over OLS, ctd}

\textbf{Third.}  Bias-variance tradeoff! {\tiny Figure from ISLR}

\begin{columns}
\column{0.7\textwidth}
\begin{figure}
\includegraphics[scale=1.1]{bias-variance-ridge}
\end{figure}

\column{0.3\textwidth}

green: variance\\~\\
black: bias\\~\\
red: total error

\end{columns}

...but how can we choose $\lambda$?  The short answer is k-fold cross validation.

\end{frame}

\begin{frame}{Model selection tradeoffs}
\begin{table}[]
\begin{tabular}{p{6cm}p{2.2cm}p{2.2cm}p{2.2cm}}
\hline
                                              & Subset selection & Ridge    & Lasso \\ \hline
Computing time             \pause                   & high             & very low & low   \\
Drives parameters to zero?      \pause              & yes              & no       & yes   \\
Parameters biased relative to ``true'' model? \pause & no               & yes      & yes   \\
Handles correlated features well?     \pause        & yes              & yes      & no    \\
Interpretability?	  \pause    & size of parameters & not really  & presence of parameters\\
 \hline
\end{tabular}
\end{table}

Lasso and ridge: 
\begin{itemize}
\item[$+$] less prediction variance than OLS, especially with many  predictors 
\item[$-$] more prediction bias than OLS
\end{itemize}
With highly correlated predictors, Lasso is unstable: indifferent between 
\begin{itemize}
\item $\hat{\beta}_1=0$ and $\hat{\beta}_2= \beta_1+\beta_2$
\item $\hat{\beta}_1= \beta_1+\beta_2$ and $\hat{\beta}_2=0$
\end{itemize}
\end{frame}

\begin{frame}{Ok, so which should I use?}

You won't know ahead of time if your model \textit{actually} needs a lot of features or not.  

\vspace{5mm}

...and it's very easy to re-run a specification with Ridge, then Lasso.

\vspace{5mm}

...so you might as well try them both.

\vspace{5mm}

Subset selection can be computationally more prohibitive, so you may not be able to run it, depending on your computing environment.

\end{frame}

\begin{frame}{Objective 5.  First, a thought experiment.}

	Consider a simple example:  Suppose you want to build a predictive model for ER visits in the East Bay
	\begin{itemize}
		\item Dependent variable, $y$: Daily ER visits at all hospitals in Oakland.
		\item Features: 
			\begin{itemize}
				\item $X_1$, Particulate matter concentration from EPA's downtown Oakland sensor, in g/m3 
				\item $X_2$, Average daily temperture at the Oakland Airport.  
			\end{itemize}
	\end{itemize}

	You use the model:
	\begin{equation*}
		y_i = \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i}
	\end{equation*}

	Suppose you fit the model with OLS as well as ridge regression (RR), with $\lambda = 0.1$.    
\end{frame}

\begin{frame}{Thought experiment, continued}

	\begin{itemize}
		\item Dependent variable, $y$: Daily ER visits at all hospitals in Oakland.
		\item Features: 
			\begin{itemize}
				\item $x_1$, Particulate matter concentration from EPA's downtown Oakland sensor, in g/m3 
				\item $x_2$, Average daily temperture at the Oakland Airport.  
			\end{itemize}
	\end{itemize}

	You realize you want to convert $x_1$ to $\mu$g/m3.  

	\vspace{5mm}

	Which model will yield different predictions with the transformed data?  The one fit with ridge or the one with OLS? \pause Answer: \textbf{Ridge!}   
	\vspace{5mm}

	In OLS, $\beta_1$ grows by $10^6$ to balance out the re-scaled of $x_1$.  
	\vspace{5mm}

	With ridge, $\lambda$ will squash $\beta_1$, meaning it won't grow as much as it did with OLS.  The predictions will change.
\end{frame}



\begin{frame}{$\Rightarrow$ Objective 5: Understand that you need to standardize variables}

For ridge \textbf{and} lasso, it's important to ``standardize" your variables:

\begin{equation}
x' = \frac{x}{\sigma_x}
\end{equation}

...and then fit the model to the normalized values.  \\~\\

Any guesses why?\\~\\

\pause

Reason: This way variables with large range don't dominate the solution.  

\end{frame}

\begin{frame}{Hot topic:  Elastic nets...}

\begin{columns}
\column{0.5\textwidth}
...are cool!\\~\\

Elastic nets
\begin{itemize}
\item Drive parameters to zero like lasso
\item Deals with correlated predictors well, like ridge (by shinking them together)
\item Give you another !\&@!\%* parameter to tune
\item Still aren't always best -- good to try several shrinkage methods, not just this.
\end{itemize}

\column{0.5\textwidth}

\begin{align*}
\lambda \sum_{k=1}^K  \left(\alpha \beta_j^2 + (1-\alpha) |\beta_j|\right)
\end{align*}


\begin{figure}
\includegraphics[scale=1]{elastic-net}
\caption*{\tiny from Elements of Statistical Learning}
\end{figure}

\end{columns}
\end{frame}

\end{document}
