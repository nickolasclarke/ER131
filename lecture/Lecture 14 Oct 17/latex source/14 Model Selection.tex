\documentclass[mathserif, aspectratio=169]{beamer}


\usepackage{movie15}
\usepackage{psfrag,graphicx}
\usepackage{amsmath}
\graphicspath{{figs/}}

\usetheme{Boadilla}
\makeatother
\setbeamertemplate{footline}[frame number]

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage{amsthm}  
\usepackage{bm}
\usepackage{lipsum}
\usepackage[linesnumbered, ruled]{algorithm2e}
\usepackage{color}
\newtheorem{assumption}{Assumptions}
\newtheorem{prop}{Proposition}
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{sol}{Decentralized Solution}
\newtheorem{thresh}{$\epsilon$-thresholding}
\definecolor{light-gray}{gray}{0.8}
\usepackage{textcomp}

\newcommand{\backupbegin}{
   \newcounter{finalframe}
   \setcounter{finalframe}{\value{framenumber}}
}
\newcommand{\backupend}{
   \setcounter{framenumber}{\value{finalframe}}
}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

\newcommand{\vs}{\vspace{5mm}}

\makeatletter
\setbeamertemplate{navigation symbols}{}


\title[Lecture 14] % (optional, use only with long paper titles)
{Data, Environment and Society: \\{Lecture 19: Model Selection and Regularization}}


%\subtitle
%{Include Only If Paper Has a Subtitle}

\author[ER131: Data, Environment and Society] 
{Instructor: Duncan Callaway\\
GSI: Salma Elmallah} 
% - Give the names in the same order as the appear in the paper.
% - Use the \inst{?} command only if the authors have different
%   affiliation.

%\logo{
%\includegraphics[width=1.5cm,height=1.5cm,keepaspectratio]{uvic_logo_h.jpg}
%}
\vspace{-20mm}
\institute[UC Berkeley] % (optional, but mostly needed)
 {\small{ \bf October 17, 2019}}


\date[October 27, 2019]

\begin{document}

\frame{
  \titlepage
}

\begin{frame}{Introduction}
We've been working a lot with this model:

\begin{equation*}
Y = \beta_0+\beta_1X_1+\cdots+\beta_pX_p+\epsilon
\end{equation*}\\~\\
And we know well how to fit it with least squares regression.\\~\\

Today's topic: how can we improve on the ``standard'' strategy of choosing coefficients via a one step application of least squares regression?\\~\\
\end{frame}

\begin{frame}{Stop using OLS?  Wait, why improve on a good thing?}
Let $n=$ number of observations, and $p=$ number of features.
 \begin{enumerate}
\item When $n$ close to $p$ in size, least squares fit can have high variance $\rightarrow$ overfitting.
\item When $p$ is big, if every variable gets a coefficient it can be hard to interpret models.  
\begin{itemize}
\item A subset of model selection known as ``feature selection'' helps get rid of some features.
\item This lets you ``throw in the kitchen sink'' with the faith that the feature selection process will get rid of it if it's not helping in prediction.
\item Note, interpreting coefficients is ok with something called subset selection (which is essentially OLS, and is next up), but not a good idea with most other machine learning methods
\end{itemize}
\end{enumerate}
\end{frame}

\begin{frame}{Ok, how do you do it?}
We'll look at two kinds of methods in Ch 6 (today and next Tuesday):
\begin{itemize}
\item Subset selection -- today
\item Shinkage -- today and tuesday
\end{itemize}

{\large \color{blue} Today's Objectives}
\begin{itemize}
\item Refine our understanding of model identification as an optimization problem
\item Learn how to adjust your errors to compare models with different numbers of predictors
\item Understand what ``regularization'' is and why we do it
\item Understand the tradeoffs between subset selection, ridge regression, and lasso
\end{itemize}


\end{frame}

\begin{frame}{Aside: $n$ choose $k$}
To aid in discussing the process it helps to understand the formula for binomial coefficients.\\~\\

When we are choosing $k$ things from a group of $n$, this formula tells us how many unique combinations of $k$ things there are.

\begin{equation}
{{n}\choose{k}} = \frac{n!}{k!(n-k)!}
\end{equation}

Simple examples:  

\begin{align*}
{{3}\choose{2}} &= \uncover<2->{\frac{3!}{2!(1!)} = 3\text{, where the set is } {(1,2), (1,3), (2,3)}}\\
{{4}\choose{2}} &= \uncover<2->{\frac{4!}{2!(2!)} = 6\text{, where the set is } {(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)}}
\end{align*}
\end{frame}

\begin{frame}{How many models?}

Say you have $p$ features and want to construct as many unique linear models as you can with just $k$ features.  \\~\\

How many models are there?\\

\pause

\begin{align*}
{p\choose{k}} = \frac{p!}{k!(p-k)!}
\end{align*}

\end{frame}

\begin{frame}{``Best'' subset selection}

1. Find $\mathcal{M}_0$ = model with $p=0$ (just a constant term...will be equal to the mean)\\~\\

2. Define $\mathcal{M}_k$ = the best model with $k$ predictors, chosen from among the $p\choose{k}$ models with $k$ predictors.  
\begin{itemize}
	\item For each $k$, choose ``best'' via something like $R^2$.  
	\item Find $\mathcal{M}_k$ for all $k$, using all the training data.\\~\\
\end{itemize}

3. Then use a model selection method like k-fold cross validation (or other adjusted error approaches like AIC) to choose from among $\mathcal{M}_0,\cdots\mathcal{M}_p$.

\pause 

\vs

\textbf{Question}:  Why could we just use $R^2$ with all the training data for step two, but not step three?

\end{frame}

\begin{frame}{When to cross-validate or use an adjusted error?}

\textbf{Question}:  Why could we just use $R^2$ with all the training data for step two, but not step three?

\vspace*{3mm}

\pause
\textbf{Answer:} Error adjustment methods and cross validation are needed to avoid overfit -- i.e. to keep you from using too many parameters.  \\~\\

In this case, overfit happens when you use too many features.\\~\\

If you're choosing from among a set of models that all have the same number of features (step two), you don't need to adjust.

\end{frame}


\begin{frame}{Is the perfect the enemy of the good?}

The ``best'' subset selection procedure is effective -- it guarantees you'll find \textit{the best} model.\\~\\

But it's not always practical to implement.  Why?

\pause

\vspace{5mm}

First, suppose $p=40$ and we're looking at models with $k = 10$ features.  Then there are

\pause

\begin{equation}
{{p}\choose{k}} = \frac{p!}{k!(p-k)!} = \frac{40!}{10!30!} = 8.5\times 10^9
\end{equation}

\vspace{5mm}

possible models with 10 features.

\vspace{5mm}

That's a lot to evaluate on a single level.

\end{frame}

\begin{frame}{Perfect v. good, ctd...}

But it gets worse!  Best subset evaluates models with \textit{every} possible number of features.

\vspace{5mm}

Imagine a binary number where each place represents whether or not a feature is included in the model.

\vspace{5mm}

For example $00101$ would be a model for which only feature 3 and 5 are included from among 5 possible features.

\vspace{5mm}

How many \textit{possible} models are there if $p=5?$

\uncover<2->{\begin{equation*}
	2^5 = 32
\end{equation*}}

If $p=40?$, this number explodes to 

\uncover<2->{\begin{equation*}
	2^{40} \approx 10^{12}
\end{equation*}}
\end{frame}

\begin{frame}{Perfect v. good, conclusion}

Why is best subset selection impractical?
\begin{itemize}
\item Computing time!  You'd need to test 1 trillion models if you had $p=40$.
\end{itemize}

\vspace{5mm}

The alternatives: ``hueristic'' algorithms that give you a strategy to search the space of models but don't guarantee that you'll get the best model.  

\vspace{5mm}

\textit{Stepwise selection} (forward selection and backward selection) can help here.
\end{frame}


%\begin{frame}
%$\mathcal{M}_0\quad \mathcal{M}_1 \quad \mathcal{M}_2 \quad \mathcal{M}_3 \quad\mathcal{M}_4$
%
%\vspace{10mm}
%
%${{4}\choose{0}}=1\quad{{4}\choose{1}}=4\quad{{4}\choose{2}}=6\quad{{4}\choose{3}}=4\quad{{4}\choose{4}}=1$
%
%\end{frame}

\begin{frame}{Stepwise selection: Forward Selection}

\begin{columns}
\column{0.5\textwidth}

\textbf{Forward selection:} Start with $\mathcal{M}_0$.  Then to choose the best model from each higher ``level'':

\vspace{5mm}
\textit{First, }within each level, add one predictor at a time to the best model from the lower level.  Use $R^2$ or other to find the best model from this set of $\mathcal{M}_{k-1} \text{``}+ 1\text{''}$

\vspace{5mm}

\textit{Second,} choose from your list $\{\mathcal{M}_0, \mathcal{M}_1,...\mathcal{M}_{p}\}$ via cross validation or adjusted error metrics.\\~\\

\column{0.5\textwidth}

\vspace*{-15mm}
\begin{center}
\only<1>{\includegraphics[scale=0.425]{4choosek_blank}}
\only<2>{\includegraphics[scale=0.425]{4choosek_filled}}
\end{center}
\end{columns}

\end{frame}

\begin{frame}{Why is Forward Selection better than Best Subset Selection?}

\pause

...because we build on the best model from the lower level ($\mathcal{M}_{k-1}$)

\vspace{5mm}

Suppose $p=10$.  
\begin{itemize}
	\item How many models will we evaluate on level 1? \pause 10. \pause
	\item How many models will we evaluate on level 2? \pause 9.
\end{itemize}

\vspace{5mm}

...For each level we are only choosing from $p+1-k$ possible models.  

\vspace{5mm}


So, for $p = 10$: 
\pause
\begin{itemize}
\item Best subset: evaluate $2^p = 1024$ models
\item Forward: evaluate $1 + 10 + 9 + 8 + 7 + ... + 1 = 56$ models
\end{itemize}



\end{frame}

\begin{frame}{Stepwise selection: Backward  Selection}

\pause
\textbf{Backward selection:} like forward selection, except that you start with $\mathcal{M}_p$ (the model with all predictors) and \textit{remove} one predictor at a time.

\end{frame}


\begin{frame}{Pop quiz}
\textbf{Challenge question:} which stepwise selection approach works in situations where $n<p$?\\~\\

\pause

\textbf{Answer:} Forward selection\\~\\

\pause

...because it naturally stops at models at level $k=n-1$.  

\vspace{5mm}

OLS can't solve models with $k>n$ features.

\end{frame}

\begin{frame}{Tracing the best model}

\begin{figure}
\includegraphics[scale=0.55]{subset_surfing}
\caption*{\tiny From Elements of Statistical Learning}
\end{figure}

Dots are performance for each possible model\\~\\

Red line is the performance for the best model in each subset size group.\\~\\

Stepwise selection would either ride on the red line or above.  
\end{frame}

% \begin{frame}{Choosing from among models with different $k$}

% As you increase the number of predictions in a model, \textit{training} data set error will naturally improve.  \\~\\\

% But this might be in part due to over-fit, i.e. soaking up irreducible error with predictors that are are spuriously correlated with the error. \\~\\

% Two approaches to deal with this problem:

% \begin{enumerate}
% \item Cross validation (see Ch 5 and notes from Class 18)
% \item ``Adjust'' the errors with $C_p$, AIC, BIC or ``Adjusted $R^2$''
% \end{enumerate}

% \end{frame}



\begin{frame}{Shrinkage methods (regularization)}

Let's begin by remembering that the processes of choosing parameters \textit{and} choosing models are rooted in ideas of optimization.\\~\\

When we run OLS, we are solving a formal optimization problem

\begin{align*}
Y &= \beta_0+\beta_1X_1+\cdots+\beta_p X_p+\epsilon & Y_i &= \sum_{k=1}^{K} X_{ik} \cdot \beta_k +\epsilon_i
\end{align*}

\begin{align*}
\Rightarrow \hat{\beta}_\text{ols} = \text{arg }\min_\beta \sum_{i=1}^N \left(Y_i - X_i \beta \right)^2
\end{align*}

To solve, set the derivatives wrt $\beta$s to zero and 

\begin{align*}
\hat{\beta}_\text{ols} &= \left(\mathbf{X}^T\mathbf{X}\right)^{-1} \left(\mathbf{X}^T\mathbf{Y}\right)
\end{align*}

\end{frame}

\begin{frame}{Subset selection as optimization}

In subset selection, you can show that you're running this optimization model:

\uncover<2->{\begin{align*}
\hat{\beta}_{SS} = \arg \min_\beta \sum_{i=1}^N \left(Y_i - X_i \beta \right)^2+\lambda \cdot \norm{\beta}_0\text{, where }\norm{\beta}_0=\sum_{k=1}^K I(\beta_k)
\end{align*}}

Here, $\norm{\cdot}_0$ is the ``zero norm,'' where $I(\cdot)$ is the `indicator function':

\uncover<2->{\begin{align*}
I(\beta)=
\begin{cases}
  1 & \text{for }\beta \ne 0\\    
  0 & \text{for }\beta = 0    
\end{cases}
\end{align*}}

\pause
Note that in this framing, $\lambda$ becomes a new parameter to choose.  High $\lambda$ favors models with fewer features and vice versa.  

\vspace{5mm}

We can use cross-validation to choose $\lambda$.  Stay tuned.

\end{frame}

\begin{frame}{Subset selection as optimization, ctd}
For those of you with experience with integer programming
\begin{itemize}
\item The formulation on the last slide is a mixed integer quadratic program
\item There are solvers for this type of problem, so you don't have to take the heuristic approaches (forward selection, etc) if you don't want to.
\item That will likely give better performance than forward or backward subset selection, but it will still be slower.
\end{itemize}
\end{frame}

\begin{frame}{Ridge regression}
In ridge regression, we use a simple modification to the subset selection objective:

\uncover<2->{\begin{align*}
\hat{\beta}_{\text{ridge}} &= \arg \min_\beta \sum_{i=1}^N \left(Y_i - X_i \beta \right)^2+\lambda \cdot \sum_{k=1}^K \beta_k^2\\
&= \arg \min_\beta \sum_{i=1}^N \left(Y_i - X_i \beta \right)^2+\lambda \cdot ||\beta||_2^2 
\end{align*}}

\pause
This leads to:

\begin{align*}
\hat{\beta}_\text{ridge} &=\left(\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I}_k\right)^{-1} \left(\mathbf{X}^T\mathbf{Y}\right)
\end{align*}

Here 
\begin{itemize}
\item $\lambda$ is a tuning parameter -- it is not unique.  
\item $\mathbf{I}_k$ is the $k\times k$ identity matrix\\~\\
\end{itemize}

\end{frame}


\begin{frame}{Ridge regression advantages}

\pause

\textbf{First}.  Suppose some of your features are linear combinations of the others.  That means you can write $x_{i,j} = Ax_{i,-j}$ for at least 1 value of $j$. \\~\\

Then $\mathbf{X}^T\mathbf{X}$ is not ``full rank'' and you can't invert it.  I.e., $(\mathbf{X}^T\mathbf{X})^{-1}$ doesn't exist.

\begin{align*}
\text{e.g., }\mathbf{X}^T\mathbf{X} = 
\begin{bmatrix}
1 & 2 & 3\\
2 & 4 & 6\\
3 & 6 & 9
\end{bmatrix}
\uncover<3->{\text{ \quad vs.  \quad}
\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I} =
\begin{bmatrix}
1+\lambda & 2 & 3\\
2 & 4+\lambda & 6\\
3 & 6 & 9+\lambda
\end{bmatrix} }
\end{align*}

\pause

But you \textit{can} invert $\left(\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I}_k\right)$.  (Since you've added a little shift to the diagonals of the matrix, which restores linear independence)\\~\\



\pause

\textbf{Second.}  Computation!  It's faster than subset selection (but solves a different problem if your objective is parameter interpretation).

\end{frame}

\begin{frame}{Ridge regression advantages, ctd}

\textbf{Third.}  Bias-variance tradeoff! {\tiny Figure from ISLR}

\begin{columns}
\column{0.5\textwidth}
\begin{figure}
\includegraphics[scale=1.1]{bias-variance-ridge}
\end{figure}

\column{0.5\textwidth}

green: variance\\
black: bias\\
red: total error

\vspace{5mm}

Tuning $\lambda$ ...
\begin{itemize}
	\item squashes down parameters associated with variables that increase model variance. 
	\item but doesn't set parameters to zero, as subset selection does.
\end{itemize}

\end{columns}
 

\pause 
...but how can we choose $\lambda$?  We'll discuss next time.
\end{frame}

\begin{frame}{Ridge regression disadvantage}

\begin{align*}
\hat{\beta}_\text{ridge} &=\left(\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I}_k\right)^{-1} \left(\mathbf{X}^T\mathbf{Y}\right)
\end{align*}

\pause
Ridge regression produces different $\beta$ estimates for different choices of $\lambda$.

\vspace{5mm}

Why is that important?
\pause
\vspace{5mm}

It means interpretation of the \textit{size} of the coefficients (i.e. inference) is problematic.

\end{frame}


\begin{frame}{Side note:  ``Regularization''}

\textbf{Regularization} Refers to the process of adding a term to the objective function of a problem that 
\begin{itemize}
\item Makes the problem ``well behaved" (easier to solve)
\item Solves a different problem from the one you originally wanted.\\~\\
\end{itemize}

In our case, the sum of squared coefficients in Ridge makes the problem very simple to solve, but we get coefficients that are biased.\\~\\

In forecasting, the reduction in output variance is more important than the growth in coefficient bias.   \\~\\

But it means we need to throw inference out the window.  


\end{frame}

\begin{frame}{The Lasso}

“Least Absolute Selection and Shrinkage Operator” (Tibshirani, 1996)\\~\\

Lasso is a simple modification of Ridge:

\uncover<2->{\begin{align*}
\min_\beta \sum_{i=1}^N \left(Y_i - X_i \beta \right)^2+\lambda \cdot \norm{\beta}_1\text{, where }\norm{\beta}_1=\sum_{k=1}^K |\beta_k|
\end{align*}}

\pause

Note, we won't find a closed-form expression for the coefficients (unlike ridge and OLS).  Why?

\pause

...Because the $\ell_1$ norm is not differentiable everywhere.

\vspace{5mm}

But you can still use your computer to find the $\hat{\beta}_\text{lasso}$ in a much more efficient way than subset selection.  

\end{frame}

\begin{frame}{Lasso tradeoffs vs OLS, subset selection and ridge regression}

\pause

\begin{enumerate}
\item[$+$] Lasso is faster to solve than subset selection
\item[$-$] Lasso is slower to solve than ridge (no analytic solution)
\item[$+$] Lasso drives parameters to zero, like subset selection and unlike ridge
\begin{itemize}
\item $\Rightarrow$ interpretability
\item But if there really are a lot of modest sized $\beta$, this is undesirable!
\end{itemize}
\item[$-$] Lasso yields biased parameters (unlike OLS / subset selection)
\item[$\pm$] Lasso similar to ridge's prediction bias-variance properties 
\begin{itemize}
\item[$+$] less prediction variance than OLS, especially with many  predictors 
\item[$-$] more prediction bias than OLS
\end{itemize}
\item[$-$] With highly correlated predictors, Lasso is unstable: indifferent between 
\begin{itemize}
\item $\hat{\beta}_1=0$ and $\hat{\beta}_2= \beta_1+\beta_2$
\item $\hat{\beta}_1= \beta_1+\beta_2$ and $\hat{\beta}_2=0$
\end{itemize}
\end{enumerate}

\end{frame}

\begin{frame}{A bit of intuition about what Lasso and ridge are doing}
\begin{columns}
\column{0.25\textwidth}
(We will go into this in much more detail next time)\\~\\

Each line is a ``contour'' for either the RSS or the regularizing penalties\\~\\

Solutions happen when the contours are tangent to each other
\column{0.7\textwidth}
\begin{figure}
\includegraphics[scale=0.6]{lasso_v_ridge}
\caption*{\tiny Figure taken from Imbens 2015 NBER lecture}
\end{figure}
\end{columns}

\vspace{5mm}

For lasso, you can see this is much more likely to happen at a corner (one parameter zero) than it is for ridge
\end{frame}


\begin{frame}{Supplemental slides}

\end{frame}


\begin{frame}{$C_p$, AIC, BIC or ``Adjusted $R^2$''}

\begin{center}
\begin{tabular}{cc}
Metric & Formula\\
\hline\hline\\
$C_p$ & $\frac{1}{n}\left(RSS + 2d\hat{\sigma}^2\right)$\\\\
AIC & $\frac{1}{n\hat{\sigma}^2}\left(RSS + 2d\hat{\sigma}^2\right)$\\\\
BIC & $\frac{1}{n\hat{\sigma}^2}\left(RSS + \log{(n)}d\hat{\sigma}^2\right)$\\\\
Adjusted $R^2$ & $1-\frac{RSS}{TSS}\frac{(n-1)}{(n-d-1)}$\\\\
\hline
\end{tabular}\\~\\
\end{center}

...where $d$ is the number of predictors and $\hat{\sigma}^2$ is your best estimate of the variance of the model error\\~\\

Note that Adjusted $R^2$ is related to the F-statistic but not the same, and that we don't \textit{always} have $0<\text{Adjusted }R^2<1$

\end{frame}

\begin{frame}{Which error adjustment to use?}

The answer is somewhat a matter of personal taste.

\begin{itemize}
\item $C_p$ and AIC are effectively the same.

\item BIC penalizes added features more than $C_p$ and AIC.

\item $C_p$, AIC and BIC have rigorous derivations.

\item $R^2$ does not have a derivation but it has an intuitive interpretation.\\~\\
\end{itemize}

My two cents:
\begin{enumerate}
\item Look at both AIC and BIC and make a judgment call if they differ
\item Use Adjusted $R^2$ if you want readers to understand the numeric value
\item But if you can afford the computing time, use cross validation instead.
\end{enumerate}
\end{frame}

\end{document}
