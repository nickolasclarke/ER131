{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab04 Extra Practice - Visualization\n",
    "\n",
    "This notebook focuses on Kernel Density Estimation (KDE). There's not much actual coding, but this lab is still has some interactive components that can help you better understand KDE.\n",
    "\n",
    "(Why KDE?)\n",
    "\n",
    "Data visualization and data smoothing are fundamental problems that data scientists battle with every day. Collected data is often discretized, but the real world is continuous. \n",
    "\n",
    "Tersely put, smoothing is the process of turning discrete data into continuous data. Data visualization is the art of converting numbers and symbols in a tabular format into a more useful and meaningful plot.\n",
    "\n",
    "Kernel Density Estimator is a function that transforms, smooths, and scales the data to be continuous. KDE Plots are useful data visualizations that can display the rough shape (aka distribution) of your population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"alameda pm2.5 data - Sheet1.csv\")\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms vs KDE Plots\n",
    "\n",
    "KDE plots are basically smoothed out histograms. Histograms are discrete, while KDE plots are continuous (making it more convenient for further analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 3 histograms with # bins of 5, 25, and 125\n",
    "\n",
    "Hint: use plt.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now take a look at the prettier seaborn histogram of Daily Mean PM2.5 Concentration.\n",
    "\n",
    "Hint: use sns.distplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.distplot(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When plotting histograms with sns.distplot, it automatically creates a Kernel Density Estimation plot, too (the smooth, blue line). The default bandwidth of this automatic KDE plot is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create KDE plots with bandwidths 1, 1/5, and 1/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does changing bins affect the histogram? How does changing bandwidth affect the KDE?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Density Estimation: Example\n",
    "\n",
    "Recall that the general formula for an estimation at a given point is:\n",
    "\n",
    "$\\sum_{obs\\in x}{K(\\dfrac{x - obs}{b})}$ where $x$ is a vector of our observed data points, $b$ is bandwidth, and $K$ is the type of Kernel function we're working with.\n",
    "\n",
    "Let's break this equation down. \n",
    "\n",
    "* $x - obs$ is saying \"shift the center of our data to be at $obs$\"\n",
    "* Bandwidth b, then, is just a scalar. $\\dfrac{x - obs}{b}$ is saying \"scale this newly centered data by $b$\"\n",
    "* K is just a function that takes in $\\dfrac{x - obs}{b}$ (which is a vector) as an input. If K is a Gaussian kernel, it will take your data input and fit it/transform it into a Gaussian curve -- centered at $x-obs$, with bandwidth $b$ as its variance.\n",
    "    * K is a probability density function, which means that the area under the curve adds up to 1.\n",
    "Tersely put, we scale our data so that it's centered at obs, scaled by bandwidth, \n",
    "\n",
    "In the following example, we will use the default Gaussian (aka normal curve) kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the purposes of this class, it's not at all necessary to understand every aspect of KDE. I find it much easier to understand what KDE does, rather than understanding what the equation does. The following example will give a high level overview of how KDE operates, which is certainly more than good enough.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Note: For simplicity, we won't be taking bandwidth into account in this example (bandwidth will be assumed to be 1). From a high level perspective, you can think of bandwidth as the width of your kernel.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start out by making up some easy data and plotting the shape of its distribution using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = [0, 0, 0, 0, 0, 0, 10, 10]\n",
    "sns.distplot(toy_data, kde = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Place a kernel at each data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "\n",
    "a = sns.rugplot(toy_data)\n",
    "\n",
    "mu = 0\n",
    "variance = 1\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "a.plot(x, stats.norm.pdf(x, mu, sigma))\n",
    "\n",
    "mu = 10\n",
    "variance = 1\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "a.plot(x, stats.norm.pdf(x, mu, sigma))\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE: There are actually 6 kernels centered at 0 (one for each 0 in toy_data), and 2 kernels centered at 10.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Sum up the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sns.rugplot(toy_data)\n",
    "\n",
    "mu = 0\n",
    "variance = 1\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "a.plot(x, 6 * stats.norm.pdf(x, mu, sigma))\n",
    "\n",
    "mu = 10\n",
    "variance = 1\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "a.plot(x, 2 * stats.norm.pdf(x, mu, sigma))\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Scale the data such that the total area under the curve is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(toy_data, kernel = 'gau')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That's cool and all, but where does bandwidth come into play?**\n",
    "\n",
    "You can think of bandwidth as the width of your kernel. With a Gaussian kernel, this is analogous to changing the variance parameter. Let's check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sns.rugplot(toy_data)\n",
    "\n",
    "mu = 0\n",
    "variance = .1 # change variance to .1 -- normal curve gets skinnier and taller\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "a.plot(x, 6 * stats.norm.pdf(x, mu, sigma)) # 6 kernels \n",
    "\n",
    "mu = 10\n",
    "variance = .1 # change variance to .1 -- normal curve gets skinnier and taller\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "a.plot(x, 2 * stats.norm.pdf(x, mu, sigma))\n",
    "a.set_title(\"Summed up Gaussian Kernels\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to sns.kdeplot\n",
    "sns.kdeplot(toy_data, bw = .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, there is a clear separation between the two bins. Because of this separation, it's now easier to think of kernel density as \"the ratio of how many kernels I can fit under each point.\" Roughly speaking, there are 6 kernels (one for each 0 in toy_data) under the first bin (0 - 2), and 2 kernels (one for each 10 in toy_data) under the second bin (8 - 10). In a ratio, it would be 3:1. \n",
    "\n",
    "(Look at the above plot again -- does it look like the bigger hump is 3 times larger than the smaller hump?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Density Estimation -- Your turn!\n",
    "\n",
    "The function below takes in a series of data, with default values of bandwidth 1, and a Gaussian kernel. Play with the data values in toy_data_custom and see what happens!\n",
    "\n",
    "(For starters: what happens if the curves overlap? Take a look at toy_data_custom = [0, 0, 0, 0, 7, 7, 10, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't really have to know every line of code in this function.\n",
    "# Just know what the entire function does and how to use it.\n",
    "# That's the beauty of abstraction!\n",
    "\n",
    "def toy_kde(data, bandwidth = 1, kernel = 'gau'):\n",
    "    \n",
    "    display(sns.distplot(data, bins = data.shape[0], kde = False))\n",
    "    \n",
    "    a = sns.rugplot(toy_data)\n",
    "\n",
    "    for mu in data.unique():\n",
    "        num_mu = sum(data == mu)\n",
    "        \n",
    "        variance = 1 * bandwidth\n",
    "        sigma = math.sqrt(variance)\n",
    "        x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "        a.plot(x, num_mu * stats.norm.pdf(x, mu, sigma))\n",
    "\n",
    "        variance = 1 * bandwidth\n",
    "        sigma = math.sqrt(variance)\n",
    "        x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "        a.plot(x, num_mu * stats.norm.pdf(x, mu, sigma))\n",
    "        a.set_title(\"Histogram of data with kernel overlay\")\n",
    "        display(a)\n",
    "    \n",
    "    f, ax = plt.subplots()\n",
    "    ax.set_title(label = \"Kernel Density Estimate Plot\")\n",
    "    display(sns.kdeplot(data, bw = bandwidth, kernel = kernel))\n",
    "    \n",
    "\n",
    "# CHANGE YOUR DATA HERE!    \n",
    "toy_data_custom = [0, 0, 0, 0, 7, 7, 10, 10] \n",
    "\n",
    "\n",
    "\n",
    "toy_kde(pd.Series(toy_data_custom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Cool$ $beans!$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Different Kernels: Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) **Let's take a look at the 10-bin histogram of the average \"Daily Mean PM2.5 Concentration\" for each \"Site ID\".**\n",
    "\n",
    "Hint: Use sns.distplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogram\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) **Let's try using a triangular kernel with bandwidth 0.5.**\n",
    "\n",
    "Hint: Use sns.kdeplot. Which argument allows you to select the kernel? Which argument is a triangular kernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE Plot using Triangle kernel\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) **Make another kdeplot again, but this time with the (default) Gaussian kernel, also with bandwidth 0.5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE Plot using Gaussian kernel\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assume our histogram from part (a) is the \"true\" distribution. Which kdeplot makes more sense?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You reached the end of the notebook :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
